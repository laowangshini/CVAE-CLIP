{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练 CLIP 文本到图像嵌入的映射网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 环境设置\n",
    "首先，导入必要的库，并配置一些基本参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# 设置随机种子以确保可重复性\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# 配置参数\n",
    "TEXT_EMBEDDINGS_PATH = './clip_embeddings/text_embeddings.pt'   # 替换为实际路径\n",
    "IMAGE_EMBEDDINGS_PATH = './clip_embeddings/image_embeddings.pt' # 替换为实际路径\n",
    "MODEL_SAVE_PATH = 'text_to_image_embedder.pth'\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# 设备设置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 定义数据集类\n",
    "创建一个自定义的 Dataset 类，用于加载预先计算的文本和图像嵌入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义自定义 Dataset 类\n",
    "class CLIPEmbedMappingDatasetPrecomputed(Dataset):\n",
    "    def __init__(self, text_embeddings_path, image_embeddings_path):\n",
    "        \"\"\"\n",
    "        初始化数据集，加载预计算的嵌入\n",
    "        :param text_embeddings_path: 文本嵌入文件路径\n",
    "        :param image_embeddings_path: 图像嵌入文件路径\n",
    "        \"\"\"\n",
    "        # 加载嵌入\n",
    "        try:\n",
    "            self.text_embeddings = torch.load(text_embeddings_path)\n",
    "            self.image_embeddings = torch.load(image_embeddings_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading embeddings: {e}\")\n",
    "            raise\n",
    "\n",
    "        # 确认嵌入数量匹配\n",
    "        assert len(self.text_embeddings) == len(self.image_embeddings), \"文本和图像嵌入的数量不匹配\"\n",
    "\n",
    "        # 确保嵌入的 dtype 为 float32\n",
    "        if self.text_embeddings.dtype != torch.float32:\n",
    "            self.text_embeddings = self.text_embeddings.float()\n",
    "            print(\"已将文本嵌入转换为 float32\")\n",
    "        if self.image_embeddings.dtype != torch.float32:\n",
    "            self.image_embeddings = self.image_embeddings.float()\n",
    "            print(\"已将图像嵌入转换为 float32\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.text_embeddings[idx], self.image_embeddings[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 定义映射网络模型\n",
    "定义一个简单的全连接神经网络，将文本嵌入映射到图像嵌入空间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义映射网络模型\n",
    "class TextToImageEmbedder(nn.Module):\n",
    "    def __init__(self, clip_dim=512, embed_dim=512):\n",
    "        super(TextToImageEmbedder, self).__init__()\n",
    "        self.mapping = nn.Sequential(\n",
    "            nn.Linear(clip_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, embed_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, text_embeddings):\n",
    "        image_embeddings = self.mapping(text_embeddings)\n",
    "        return image_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 创建数据加载器\n",
    "使用自定义的 Dataset 类创建 DataLoader，以便在训练过程中批量加载数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6208/2941295110.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.text_embeddings = torch.load(text_embeddings_path)\n",
      "/tmp/ipykernel_6208/2941295110.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.image_embeddings = torch.load(image_embeddings_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已将文本嵌入转换为 float32\n",
      "已将图像嵌入转换为 float32\n",
      "数据集大小: 162770\n"
     ]
    }
   ],
   "source": [
    "# 创建映射网络的数据集和数据加载器（使用预计算嵌入）\n",
    "try:\n",
    "    mapping_dataset = CLIPEmbedMappingDatasetPrecomputed(\n",
    "        text_embeddings_path=TEXT_EMBEDDINGS_PATH,\n",
    "        image_embeddings_path=IMAGE_EMBEDDINGS_PATH\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing mapping dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# 确认数据集大小\n",
    "print(f\"数据集大小: {len(mapping_dataset)}\")\n",
    "\n",
    "# 创建 DataLoader\n",
    "mapping_loader = DataLoader(\n",
    "    mapping_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # 根据需要调整\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 初始化模型、损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建映射网络模型\n",
    "embedder = TextToImageEmbedder(clip_dim=512, embed_dim=512).to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion_mapping = nn.MSELoss()\n",
    "optimizer_mapping = optim.Adam(embedder.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 训练映射网络\n",
    "定义训练函数，并开始训练过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 1/20: 100%|██████████| 1272/1272 [00:03<00:00, 413.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 1, 平均损失: 0.000563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 2/20: 100%|██████████| 1272/1272 [00:02<00:00, 463.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 2, 平均损失: 0.000514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 3/20: 100%|██████████| 1272/1272 [00:02<00:00, 471.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 3, 平均损失: 0.000507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 4/20: 100%|██████████| 1272/1272 [00:02<00:00, 468.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 4, 平均损失: 0.000502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 5/20: 100%|██████████| 1272/1272 [00:02<00:00, 490.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 5, 平均损失: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 6/20: 100%|██████████| 1272/1272 [00:02<00:00, 438.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 6, 平均损失: 0.000498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 7/20: 100%|██████████| 1272/1272 [00:02<00:00, 538.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 7, 平均损失: 0.000496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 8/20: 100%|██████████| 1272/1272 [00:02<00:00, 491.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 8, 平均损失: 0.000495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 9/20: 100%|██████████| 1272/1272 [00:02<00:00, 534.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 9, 平均损失: 0.000494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 10/20: 100%|██████████| 1272/1272 [00:02<00:00, 543.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 10, 平均损失: 0.000493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 11/20: 100%|██████████| 1272/1272 [00:02<00:00, 538.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 11, 平均损失: 0.000492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 12/20: 100%|██████████| 1272/1272 [00:02<00:00, 473.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 12, 平均损失: 0.000491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 13/20: 100%|██████████| 1272/1272 [00:02<00:00, 472.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 13, 平均损失: 0.000491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 14/20: 100%|██████████| 1272/1272 [00:02<00:00, 481.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 14, 平均损失: 0.000490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 15/20: 100%|██████████| 1272/1272 [00:02<00:00, 540.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 15, 平均损失: 0.000489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 16/20: 100%|██████████| 1272/1272 [00:02<00:00, 506.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 16, 平均损失: 0.000489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 17/20: 100%|██████████| 1272/1272 [00:02<00:00, 522.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 17, 平均损失: 0.000488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 18/20: 100%|██████████| 1272/1272 [00:02<00:00, 556.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 18, 平均损失: 0.000488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 19/20: 100%|██████████| 1272/1272 [00:02<00:00, 561.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 19, 平均损失: 0.000487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 20/20: 100%|██████████| 1272/1272 [00:02<00:00, 499.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 20, 平均损失: 0.000487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 定义训练函数\n",
    "def train_mapping_network(model, loader, optimizer, criterion, num_epochs=20):\n",
    "    model.train()\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        epoch_loss = 0\n",
    "        for text_emb, img_emb in tqdm(loader, desc=f\"Mapping Epoch {epoch}/{num_epochs}\"):\n",
    "            # 将嵌入移动到GPU\n",
    "            text_emb = text_emb.to(device)\n",
    "            img_emb = img_emb.to(device)\n",
    "\n",
    "            # 前向传播\n",
    "            pred_img_emb = model(text_emb)\n",
    "            loss = criterion(pred_img_emb, img_emb)\n",
    "\n",
    "            # 反向传播和优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(loader.dataset)\n",
    "        print(f\"Mapping Epoch {epoch}, 平均损失: {avg_loss:.6f}\")\n",
    "    return model\n",
    "\n",
    "# 开始训练\n",
    "trained_embedder = train_mapping_network(embedder, mapping_loader, optimizer_mapping, criterion_mapping, num_epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "映射网络已保存为: text_to_image_embedder.pth\n"
     ]
    }
   ],
   "source": [
    "# 保存映射网络\n",
    "torch.save(trained_embedder.state_dict(), MODEL_SAVE_PATH)\n",
    "print(f\"映射网络已保存为: {MODEL_SAVE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用后来生成的嵌入训练了一个新的网络效果比较好\n",
    "唯一用到的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n",
      "已将文本嵌入转换为 float32\n",
      "已将图像嵌入转换为 float32\n",
      "数据集大小: 162770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 1/20: 100%|██████████| 1272/1272 [00:04<00:00, 271.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 1, 平均损失: 0.001742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 2/20: 100%|██████████| 1272/1272 [00:04<00:00, 311.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 2, 平均损失: 0.001429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 3/20: 100%|██████████| 1272/1272 [00:04<00:00, 307.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 3, 平均损失: 0.001401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 4/20: 100%|██████████| 1272/1272 [00:04<00:00, 313.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 4, 平均损失: 0.001386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 5/20: 100%|██████████| 1272/1272 [00:03<00:00, 340.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 5, 平均损失: 0.001377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 6/20: 100%|██████████| 1272/1272 [00:04<00:00, 317.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 6, 平均损失: 0.001370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 7/20: 100%|██████████| 1272/1272 [00:04<00:00, 281.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 7, 平均损失: 0.001365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 8/20: 100%|██████████| 1272/1272 [00:03<00:00, 326.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 8, 平均损失: 0.001361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 9/20: 100%|██████████| 1272/1272 [00:03<00:00, 323.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 9, 平均损失: 0.001357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 10/20: 100%|██████████| 1272/1272 [00:04<00:00, 315.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 10, 平均损失: 0.001354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 11/20: 100%|██████████| 1272/1272 [00:04<00:00, 297.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 11, 平均损失: 0.001351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 12/20: 100%|██████████| 1272/1272 [00:04<00:00, 309.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 12, 平均损失: 0.001349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 13/20: 100%|██████████| 1272/1272 [00:04<00:00, 315.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 13, 平均损失: 0.001347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 14/20: 100%|██████████| 1272/1272 [00:03<00:00, 333.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 14, 平均损失: 0.001345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 15/20: 100%|██████████| 1272/1272 [00:03<00:00, 325.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 15, 平均损失: 0.001342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 16/20: 100%|██████████| 1272/1272 [00:04<00:00, 313.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 16, 平均损失: 0.001341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 17/20: 100%|██████████| 1272/1272 [00:04<00:00, 311.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 17, 平均损失: 0.001339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 18/20: 100%|██████████| 1272/1272 [00:04<00:00, 309.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 18, 平均损失: 0.001338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 19/20: 100%|██████████| 1272/1272 [00:04<00:00, 298.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 19, 平均损失: 0.001336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 20/20: 100%|██████████| 1272/1272 [00:04<00:00, 308.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 20, 平均损失: 0.001335\n",
      "映射网络已保存为: text_to_image_embedder.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# 设置随机种子以确保可重复性\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# 配置参数\n",
    "TEXT_EMBEDDINGS_PATH = '/root/autodl-tmp/clip_embeddings/text_embeddings_partition_0.pt'   # 替换为实际路径\n",
    "IMAGE_EMBEDDINGS_PATH = '/root/autodl-tmp/clip_embeddings/image_embeddings_partition_0.pt' # 替换为实际路径\n",
    "MODEL_SAVE_PATH = 'text_to_image_embedder.pth'\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# 设备设置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 定义自定义 Dataset 类\n",
    "class CLIPEmbedMappingDatasetPrecomputed(Dataset):\n",
    "    def __init__(self, text_embeddings_path, image_embeddings_path):\n",
    "        \"\"\"\n",
    "        初始化数据集，加载预计算的嵌入\n",
    "        :param text_embeddings_path: 文本嵌入文件路径\n",
    "        :param image_embeddings_path: 图像嵌入文件路径\n",
    "        \"\"\"\n",
    "        # 加载嵌入\n",
    "        try:\n",
    "            self.text_embeddings = torch.load(text_embeddings_path)\n",
    "            self.image_embeddings = torch.load(image_embeddings_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading embeddings: {e}\")\n",
    "            raise\n",
    "\n",
    "        # 确认嵌入数量匹配\n",
    "        assert len(self.text_embeddings) == len(self.image_embeddings), \"文本和图像嵌入的数量不匹配\"\n",
    "\n",
    "        # 确保嵌入的 dtype 为 float32\n",
    "        if self.text_embeddings.dtype != torch.float32:\n",
    "            self.text_embeddings = self.text_embeddings.float()\n",
    "            print(\"已将文本嵌入转换为 float32\")\n",
    "        if self.image_embeddings.dtype != torch.float32:\n",
    "            self.image_embeddings = self.image_embeddings.float()\n",
    "            print(\"已将图像嵌入转换为 float32\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.text_embeddings[idx], self.image_embeddings[idx]\n",
    "\n",
    "# 定义优化后的映射网络模型\n",
    "class TextToImageEmbedder(nn.Module):\n",
    "    def __init__(self, clip_dim=512, embed_dim=512):\n",
    "        super(TextToImageEmbedder, self).__init__()\n",
    "        self.mapping = nn.Sequential(\n",
    "            nn.Linear(clip_dim, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, embed_dim),\n",
    "            nn.BatchNorm1d(embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, embed_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, text_embeddings):\n",
    "        image_embeddings = self.mapping(text_embeddings)\n",
    "        return image_embeddings\n",
    "\n",
    "# 定义新的余弦相似度损失函数\n",
    "class CosineSimilarityLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CosineSimilarityLoss, self).__init__()\n",
    "        self.cos = nn.CosineSimilarity(dim=1)\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # 余弦相似度范围在 [-1, 1]，我们希望最大化相似度，因此损失为 1 - cos_sim\n",
    "        return 1 - self.cos(pred, target).mean()\n",
    "\n",
    "# 创建映射网络的数据集和数据加载器（使用预计算嵌入）\n",
    "try:\n",
    "    mapping_dataset = CLIPEmbedMappingDatasetPrecomputed(\n",
    "        text_embeddings_path=TEXT_EMBEDDINGS_PATH,\n",
    "        image_embeddings_path=IMAGE_EMBEDDINGS_PATH\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing mapping dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# 确认数据集大小\n",
    "print(f\"数据集大小: {len(mapping_dataset)}\")\n",
    "\n",
    "# 创建 DataLoader\n",
    "mapping_loader = DataLoader(\n",
    "    mapping_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,  # 根据需要调整\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# 创建映射网络模型\n",
    "embedder = TextToImageEmbedder(clip_dim=512, embed_dim=512).to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion_mapping = CosineSimilarityLoss()\n",
    "optimizer_mapping = optim.Adam(embedder.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# 定义训练函数\n",
    "def train_mapping_network(model, loader, optimizer, criterion, num_epochs=20):\n",
    "    model.train()\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        epoch_loss = 0\n",
    "        for text_emb, img_emb in tqdm(loader, desc=f\"Mapping Epoch {epoch}/{num_epochs}\"):\n",
    "            # 将嵌入移动到GPU\n",
    "            text_emb = text_emb.to(device)\n",
    "            img_emb = img_emb.to(device)\n",
    "\n",
    "            # 前向传播\n",
    "            pred_img_emb = model(text_emb)\n",
    "            loss = criterion(pred_img_emb, img_emb)\n",
    "\n",
    "            # 反向传播和优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(loader.dataset)\n",
    "        print(f\"Mapping Epoch {epoch}, 平均损失: {avg_loss:.6f}\")\n",
    "    return model\n",
    "\n",
    "# 开始训练\n",
    "trained_embedder = train_mapping_network(embedder, mapping_loader, optimizer_mapping, criterion_mapping, num_epochs=NUM_EPOCHS)\n",
    "\n",
    "# 保存映射网络\n",
    "torch.save(trained_embedder.state_dict(), MODEL_SAVE_PATH)\n",
    "print(f\"映射网络已保存为: {MODEL_SAVE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新的更深的网络 不行 余弦相似度反而降低了\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n",
      "已将文本嵌入转换为 float32\n",
      "已将图像嵌入转换为 float32\n",
      "数据集大小: 162770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 1/20: 100%|██████████| 1272/1272 [00:06<00:00, 211.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 1, 平均损失: 0.001736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 2/20: 100%|██████████| 1272/1272 [00:05<00:00, 217.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 2, 平均损失: 0.001431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 3/20: 100%|██████████| 1272/1272 [00:05<00:00, 239.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 3, 平均损失: 0.001401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 4/20: 100%|██████████| 1272/1272 [00:05<00:00, 242.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 4, 平均损失: 0.001386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 5/20: 100%|██████████| 1272/1272 [00:05<00:00, 217.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 5, 平均损失: 0.001376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 6/20: 100%|██████████| 1272/1272 [00:06<00:00, 207.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 6, 平均损失: 0.001368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 7/20: 100%|██████████| 1272/1272 [00:06<00:00, 207.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 7, 平均损失: 0.001363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 8/20: 100%|██████████| 1272/1272 [00:05<00:00, 218.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 8, 平均损失: 0.001358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 9/20: 100%|██████████| 1272/1272 [00:05<00:00, 219.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 9, 平均损失: 0.001354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 10/20: 100%|██████████| 1272/1272 [00:05<00:00, 231.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 10, 平均损失: 0.001351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 11/20: 100%|██████████| 1272/1272 [00:05<00:00, 230.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 11, 平均损失: 0.001348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 12/20: 100%|██████████| 1272/1272 [00:05<00:00, 222.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 12, 平均损失: 0.001345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 13/20: 100%|██████████| 1272/1272 [00:05<00:00, 214.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 13, 平均损失: 0.001343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 14/20: 100%|██████████| 1272/1272 [00:05<00:00, 223.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 14, 平均损失: 0.001340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 15/20: 100%|██████████| 1272/1272 [00:05<00:00, 222.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 15, 平均损失: 0.001338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 16/20: 100%|██████████| 1272/1272 [00:06<00:00, 205.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 16, 平均损失: 0.001335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 17/20: 100%|██████████| 1272/1272 [00:06<00:00, 204.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 17, 平均损失: 0.001333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 18/20: 100%|██████████| 1272/1272 [00:05<00:00, 217.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 18, 平均损失: 0.001331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 19/20: 100%|██████████| 1272/1272 [00:05<00:00, 214.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 19, 平均损失: 0.001330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 20/20: 100%|██████████| 1272/1272 [00:06<00:00, 210.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 20, 平均损失: 0.001327\n",
      "映射网络已保存为: text_to_image_embedder_v2.pth\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# 设置随机种子以确保可重复性\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# 配置参数\n",
    "TEXT_EMBEDDINGS_PATH = '/root/autodl-tmp/clip_embeddings/text_embeddings_partition_0.pt'   # 替换为实际路径\n",
    "IMAGE_EMBEDDINGS_PATH = '/root/autodl-tmp/clip_embeddings/image_embeddings_partition_0.pt' # 替换为实际路径\n",
    "MODEL_SAVE_PATH = 'text_to_image_embedder_v2.pth' # 修改了保存文件名，以区分不同版本的模型\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# 设备设置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 定义自定义 Dataset 类\n",
    "class CLIPEmbedMappingDatasetPrecomputed(Dataset):\n",
    "    def __init__(self, text_embeddings_path, image_embeddings_path):\n",
    "        \"\"\"\n",
    "        初始化数据集，加载预计算的嵌入\n",
    "        :param text_embeddings_path: 文本嵌入文件路径\n",
    "        :param image_embeddings_path: 图像嵌入文件路径\n",
    "        \"\"\"\n",
    "        # 加载嵌入\n",
    "        try:\n",
    "            self.text_embeddings = torch.load(text_embeddings_path)\n",
    "            self.image_embeddings = torch.load(image_embeddings_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading embeddings: {e}\")\n",
    "            raise\n",
    "\n",
    "        # 确认嵌入数量匹配\n",
    "        assert len(self.text_embeddings) == len(self.image_embeddings), \"文本和图像嵌入的数量不匹配\"\n",
    "\n",
    "        # 确保嵌入的 dtype 为 float32\n",
    "        if self.text_embeddings.dtype != torch.float32:\n",
    "            self.text_embeddings = self.text_embeddings.float()\n",
    "            print(\"已将文本嵌入转换为 float32\")\n",
    "        if self.image_embeddings.dtype != torch.float32:\n",
    "            self.image_embeddings = self.image_embeddings.float()\n",
    "            print(\"已将图像嵌入转换为 float32\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.text_embeddings[idx], self.image_embeddings[idx]\n",
    "\n",
    "# 定义优化后的映射网络模型 V2 (增加深度和宽度)\n",
    "class TextToImageEmbedderV2(nn.Module): # V2 表示 Version 2\n",
    "    def __init__(self, clip_dim=512, embed_dim=512):\n",
    "        super(TextToImageEmbedderV2, self).__init__()\n",
    "        self.mapping = nn.Sequential(\n",
    "            nn.Linear(clip_dim, 4096), # 加宽到 4096\n",
    "            nn.BatchNorm1d(4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(4096, 2048), # 加宽到 2048\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512), # 增加一个 512 维度的隐藏层\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, embed_dim) # 输出层保持不变\n",
    "        )\n",
    "\n",
    "    def forward(self, text_embeddings):\n",
    "        image_embeddings = self.mapping(text_embeddings)\n",
    "        return image_embeddings\n",
    "\n",
    "\n",
    "# 定义新的余弦相似度损失函数\n",
    "class CosineSimilarityLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CosineSimilarityLoss, self).__init__()\n",
    "        self.cos = nn.CosineSimilarity(dim=1)\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # 余弦相似度范围在 [-1, 1]，我们希望最大化相似度，因此损失为 1 - cos_sim\n",
    "        return 1 - self.cos(pred, target).mean()\n",
    "\n",
    "# 创建映射网络的数据集和数据加载器（使用预计算嵌入）\n",
    "try:\n",
    "    mapping_dataset = CLIPEmbedMappingDatasetPrecomputed(\n",
    "        text_embeddings_path=TEXT_EMBEDDINGS_PATH,\n",
    "        image_embeddings_path=IMAGE_EMBEDDINGS_PATH\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing mapping dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# 确认数据集大小\n",
    "print(f\"数据集大小: {len(mapping_dataset)}\")\n",
    "\n",
    "# 创建 DataLoader\n",
    "mapping_loader = DataLoader(\n",
    "    mapping_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,  # 根据需要调整\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# 创建映射网络模型 V2 (使用加深加宽的模型)\n",
    "embedder = TextToImageEmbedderV2(clip_dim=512, embed_dim=512).to(device) # 使用 TextToImageEmbedderV2\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion_mapping = CosineSimilarityLoss()\n",
    "optimizer_mapping = optim.Adam(embedder.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# 定义训练函数\n",
    "def train_mapping_network(model, loader, optimizer, criterion, num_epochs=20):\n",
    "    model.train()\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        epoch_loss = 0\n",
    "        for text_emb, img_emb in tqdm(loader, desc=f\"Mapping Epoch {epoch}/{num_epochs}\"):\n",
    "            # 将嵌入移动到GPU\n",
    "            text_emb = text_emb.to(device)\n",
    "            img_emb = img_emb.to(device)\n",
    "\n",
    "            # 前向传播\n",
    "            pred_img_emb = model(text_emb)\n",
    "            loss = criterion(pred_img_emb, img_emb)\n",
    "\n",
    "            # 反向传播和优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(loader.dataset)\n",
    "        print(f\"Mapping Epoch {epoch}, 平均损失: {avg_loss:.6f}\")\n",
    "    return model\n",
    "\n",
    "# 开始训练\n",
    "trained_embedder = train_mapping_network(embedder, mapping_loader, optimizer_mapping, criterion_mapping, num_epochs=NUM_EPOCHS)\n",
    "\n",
    "# 保存映射网络\n",
    "torch.save(trained_embedder.state_dict(), MODEL_SAVE_PATH)\n",
    "print(f\"映射网络已保存为: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新的 图片太模糊 不行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n",
      "已将文本嵌入转换为 float32\n",
      "已将图像嵌入转换为 float32\n",
      "数据集大小: 162770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 1/20: 100%|██████████| 1272/1272 [00:05<00:00, 219.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 1, 平均损失: 0.001952, Learning Rate: 0.00004969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 2/20: 100%|██████████| 1272/1272 [00:06<00:00, 210.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 2, 平均损失: 0.001475, Learning Rate: 0.00004878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 3/20: 100%|██████████| 1272/1272 [00:06<00:00, 203.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 3, 平均损失: 0.001431, Learning Rate: 0.00004728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 4/20: 100%|██████████| 1272/1272 [00:05<00:00, 213.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 4, 平均损失: 0.001408, Learning Rate: 0.00004523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 5/20: 100%|██████████| 1272/1272 [00:05<00:00, 212.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 5, 平均损失: 0.001393, Learning Rate: 0.00004268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 6/20: 100%|██████████| 1272/1272 [00:06<00:00, 211.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 6, 平均损失: 0.001383, Learning Rate: 0.00003969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 7/20: 100%|██████████| 1272/1272 [00:05<00:00, 230.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 7, 平均损失: 0.001376, Learning Rate: 0.00003635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 8/20: 100%|██████████| 1272/1272 [00:05<00:00, 249.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 8, 平均损失: 0.001370, Learning Rate: 0.00003273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 9/20: 100%|██████████| 1272/1272 [00:05<00:00, 241.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 9, 平均损失: 0.001365, Learning Rate: 0.00002891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 10/20: 100%|██████████| 1272/1272 [00:05<00:00, 221.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 10, 平均损失: 0.001360, Learning Rate: 0.00002500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 11/20: 100%|██████████| 1272/1272 [00:05<00:00, 223.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 11, 平均损失: 0.001357, Learning Rate: 0.00002109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 12/20: 100%|██████████| 1272/1272 [00:05<00:00, 230.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 12, 平均损失: 0.001353, Learning Rate: 0.00001727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 13/20: 100%|██████████| 1272/1272 [00:05<00:00, 228.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 13, 平均损失: 0.001350, Learning Rate: 0.00001365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 14/20: 100%|██████████| 1272/1272 [00:05<00:00, 221.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 14, 平均损失: 0.001348, Learning Rate: 0.00001031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 15/20: 100%|██████████| 1272/1272 [00:05<00:00, 235.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 15, 平均损失: 0.001345, Learning Rate: 0.00000732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 16/20: 100%|██████████| 1272/1272 [00:05<00:00, 225.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 16, 平均损失: 0.001343, Learning Rate: 0.00000477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 17/20: 100%|██████████| 1272/1272 [00:05<00:00, 220.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 17, 平均损失: 0.001342, Learning Rate: 0.00000272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 18/20: 100%|██████████| 1272/1272 [00:05<00:00, 226.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 18, 平均损失: 0.001340, Learning Rate: 0.00000122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 19/20: 100%|██████████| 1272/1272 [00:05<00:00, 237.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 19, 平均损失: 0.001340, Learning Rate: 0.00000031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 20/20: 100%|██████████| 1272/1272 [00:05<00:00, 219.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Epoch 20, 平均损失: 0.001339, Learning Rate: 0.00000000\n",
      "映射网络已保存为: text_to_image_embedder_v3.pth\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# 设置随机种子以确保可重复性\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# 配置参数\n",
    "TEXT_EMBEDDINGS_PATH = '/root/autodl-tmp/clip_embeddings/text_embeddings_partition_0.pt'   # 替换为实际路径\n",
    "IMAGE_EMBEDDINGS_PATH = '/root/autodl-tmp/clip_embeddings/image_embeddings_partition_0.pt' # 替换为实际路径\n",
    "MODEL_SAVE_PATH = 'text_to_image_embedder_v3.pth' # 修改了保存文件名，以区分不同版本的模型\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 5e-5  # 降低学习率\n",
    "WEIGHT_DECAY = 1e-5    # 添加 L2 正则化\n",
    "\n",
    "# 设备设置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 定义自定义 Dataset 类\n",
    "class CLIPEmbedMappingDatasetPrecomputed(Dataset):\n",
    "    def __init__(self, text_embeddings_path, image_embeddings_path):\n",
    "        \"\"\"\n",
    "        初始化数据集，加载预计算的嵌入\n",
    "        :param text_embeddings_path: 文本嵌入文件路径\n",
    "        :param image_embeddings_path: 图像嵌入文件路径\n",
    "        \"\"\"\n",
    "        # 加载嵌入\n",
    "        try:\n",
    "            self.text_embeddings = torch.load(text_embeddings_path)\n",
    "            self.image_embeddings = torch.load(image_embeddings_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading embeddings: {e}\")\n",
    "            raise\n",
    "\n",
    "        # 确认嵌入数量匹配\n",
    "        assert len(self.text_embeddings) == len(self.image_embeddings), \"文本和图像嵌入的数量不匹配\"\n",
    "\n",
    "        # 确保嵌入的 dtype 为 float32\n",
    "        if self.text_embeddings.dtype != torch.float32:\n",
    "            self.text_embeddings = self.text_embeddings.float()\n",
    "            print(\"已将文本嵌入转换为 float32\")\n",
    "        if self.image_embeddings.dtype != torch.float32:\n",
    "            self.image_embeddings = self.image_embeddings.float()\n",
    "            print(\"已将图像嵌入转换为 float32\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.text_embeddings[idx], self.image_embeddings[idx]\n",
    "\n",
    "# 定义优化后的映射网络模型 V2 (增加深度和宽度)\n",
    "class TextToImageEmbedderV2(nn.Module): # V2 表示 Version 2\n",
    "    def __init__(self, clip_dim=512, embed_dim=512):\n",
    "        super(TextToImageEmbedderV2, self).__init__()\n",
    "        self.mapping = nn.Sequential(\n",
    "            nn.Linear(clip_dim, 4096), # 加宽到 4096\n",
    "            nn.BatchNorm1d(4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(4096, 2048), # 加宽到 2048\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512), # 增加一个 512 维度的隐藏层\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, embed_dim) # 输出层保持不变\n",
    "        )\n",
    "\n",
    "    def forward(self, text_embeddings):\n",
    "        image_embeddings = self.mapping(text_embeddings)\n",
    "        return image_embeddings\n",
    "\n",
    "\n",
    "# 定义新的余弦相似度损失函数\n",
    "class CosineSimilarityLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CosineSimilarityLoss, self).__init__()\n",
    "        self.cos = nn.CosineSimilarity(dim=1)\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # 余弦相似度范围在 [-1, 1]，我们希望最大化相似度，因此损失为 1 - cos_sim\n",
    "        return 1 - self.cos(pred, target).mean()\n",
    "\n",
    "# 创建映射网络的数据集和数据加载器（使用预计算嵌入）\n",
    "try:\n",
    "    mapping_dataset = CLIPEmbedMappingDatasetPrecomputed(\n",
    "        text_embeddings_path=TEXT_EMBEDDINGS_PATH,\n",
    "        image_embeddings_path=IMAGE_EMBEDDINGS_PATH\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing mapping dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# 确认数据集大小\n",
    "print(f\"数据集大小: {len(mapping_dataset)}\")\n",
    "\n",
    "# 创建 DataLoader\n",
    "mapping_loader = DataLoader(\n",
    "    mapping_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,  # 根据需要调整\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# 创建映射网络模型 V2 (使用加深加宽的模型)\n",
    "embedder = TextToImageEmbedderV2(clip_dim=512, embed_dim=512).to(device) # 使用 TextToImageEmbedderV2\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion_mapping = CosineSimilarityLoss()\n",
    "optimizer_mapping = optim.Adam(embedder.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY) # 添加 weight_decay\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_mapping, T_max=NUM_EPOCHS) # Cosine Annealing Scheduler\n",
    "\n",
    "# 定义训练函数\n",
    "def train_mapping_network(model, loader, optimizer, criterion, num_epochs=20, scheduler=None): # 添加 scheduler 参数\n",
    "    model.train()\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        epoch_loss = 0\n",
    "        for text_emb, img_emb in tqdm(loader, desc=f\"Mapping Epoch {epoch}/{num_epochs}\"):\n",
    "            # 将嵌入移动到GPU\n",
    "            text_emb = text_emb.to(device)\n",
    "            img_emb = img_emb.to(device)\n",
    "\n",
    "            # 前向传播\n",
    "            pred_img_emb = model(text_emb)\n",
    "            loss = criterion(pred_img_emb, img_emb)\n",
    "\n",
    "            # 反向传播和优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step() # 更新学习率\n",
    "\n",
    "        avg_loss = epoch_loss / len(loader.dataset)\n",
    "        current_lr = optimizer.param_groups[0]['lr'] # 获取当前学习率\n",
    "        print(f\"Mapping Epoch {epoch}, 平均损失: {avg_loss:.6f}, Learning Rate: {current_lr:.8f}\") # 打印学习率\n",
    "    return model\n",
    "\n",
    "# 开始训练\n",
    "trained_embedder = train_mapping_network(embedder, mapping_loader, optimizer_mapping, criterion_mapping, num_epochs=NUM_EPOCHS, scheduler=scheduler) # 传入 scheduler\n",
    "\n",
    "# 保存映射网络\n",
    "torch.save(trained_embedder.state_dict(), MODEL_SAVE_PATH)\n",
    "print(f\"映射网络已保存为: {MODEL_SAVE_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorc_test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
