{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全部重新理一遍\n",
    "### 我们需要什么CLIP根据原有的提示词生成的嵌入 我们用attr标签生成了对应的text_imbeddings,利用图片本身生成了image_imbeddings 其中还分割了图片本身\n",
    "在preprocess_clip_embeddings.ipynb中第一个就是（我觉得正确着呢）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用上述生成的图像嵌入和文本嵌入训练出我们的映射网络\n",
    "这时候就可以评测映射网络了，在文件text_to_image_embedder.ipynb中\n",
    "唯一用到的（是第二个）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tsne-后来的对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Text embeddings data type: torch.float16\n",
      "Image embeddings data type: torch.float16\n",
      "Converted text embeddings to float32\n",
      "Converted image embeddings to float32\n",
      "Generating predicted image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 636/636 [00:00<00:00, 1473.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing cosine similarity...\n",
      "Average cosine similarity: 0.8324 ± 0.0507\n",
      "Performing t-SNE dimensionality reduction...\n",
      "Analysis complete! Results saved to ./comparison_results directory\n"
     ]
    }
   ],
   "source": [
    "# 导入所需库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# 定义映射网络模型（确保与训练时使用的结构相同）\n",
    "class TextToImageEmbedder(nn.Module):\n",
    "    def __init__(self, clip_dim=512, embed_dim=512):\n",
    "        super(TextToImageEmbedder, self).__init__()\n",
    "        self.mapping = nn.Sequential(\n",
    "            nn.Linear(clip_dim, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, embed_dim),\n",
    "            nn.BatchNorm1d(embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, embed_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, text_embeddings):\n",
    "        image_embeddings = self.mapping(text_embeddings)\n",
    "        return image_embeddings\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 定义文件路径\n",
    "MODEL_PATH = 'text_to_image_embedder.pth'\n",
    "TEXT_EMBEDDINGS_PATH = './clip_embeddings/text_embeddings.pt'\n",
    "IMAGE_EMBEDDINGS_PATH = './clip_embeddings/image_embeddings.pt'\n",
    "OUTPUT_DIR = './comparison_results'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 加载嵌入\n",
    "text_embeddings = torch.load(TEXT_EMBEDDINGS_PATH)\n",
    "true_image_embeddings = torch.load(IMAGE_EMBEDDINGS_PATH)\n",
    "\n",
    "# 检查和转换数据类型\n",
    "print(f\"Text embeddings data type: {text_embeddings.dtype}\")\n",
    "print(f\"Image embeddings data type: {true_image_embeddings.dtype}\")\n",
    "\n",
    "# 确保两者都是 float32 类型\n",
    "if text_embeddings.dtype != torch.float32:\n",
    "    text_embeddings = text_embeddings.float()\n",
    "    print(\"Converted text embeddings to float32\")\n",
    "if true_image_embeddings.dtype != torch.float32:\n",
    "    true_image_embeddings = true_image_embeddings.float()\n",
    "    print(\"Converted image embeddings to float32\")\n",
    "\n",
    "# 加载模型\n",
    "embedder = TextToImageEmbedder().to(device)\n",
    "embedder.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "embedder.eval()\n",
    "\n",
    "# 使用训练好的映射网络生成图像嵌入\n",
    "print(\"Generating predicted image embeddings...\")\n",
    "with torch.no_grad():\n",
    "    # 分批处理以减少内存使用\n",
    "    batch_size = 256\n",
    "    pred_image_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(text_embeddings), batch_size)):\n",
    "        batch = text_embeddings[i:i+batch_size].to(device)\n",
    "        preds = embedder(batch).cpu()\n",
    "        pred_image_embeddings.append(preds)\n",
    "    \n",
    "    pred_image_embeddings = torch.cat(pred_image_embeddings, dim=0)\n",
    "\n",
    "# 确保嵌入向量已归一化（对于余弦相似度计算）\n",
    "pred_image_embeddings_norm = F.normalize(pred_image_embeddings, p=2, dim=1)\n",
    "true_image_embeddings_norm = F.normalize(true_image_embeddings, p=2, dim=1)\n",
    "\n",
    "# 计算余弦相似度\n",
    "print(\"Computing cosine similarity...\")\n",
    "cos_sim = torch.sum(pred_image_embeddings_norm * true_image_embeddings_norm, dim=1)\n",
    "mean_cos_sim = cos_sim.mean().item()\n",
    "std_cos_sim = cos_sim.std().item()\n",
    "\n",
    "print(f\"Average cosine similarity: {mean_cos_sim:.4f} ± {std_cos_sim:.4f}\")\n",
    "\n",
    "# 保存相似度分布直方图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(cos_sim.numpy(), bins=50, alpha=0.7)\n",
    "plt.axvline(mean_cos_sim, color='r', linestyle='--', label=f'Mean: {mean_cos_sim:.4f}')\n",
    "plt.title('Cosine Similarity Distribution Between Predicted and Real Image Embeddings')\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'cosine_similarity_histogram.png'))\n",
    "plt.close()\n",
    "\n",
    "# 使用t-SNE可视化嵌入\n",
    "print(\"Performing t-SNE dimensionality reduction...\")\n",
    "\n",
    "# 为了提高效率，随机采样部分数据点（如果数据集很大）\n",
    "sample_size = min(5000, len(pred_image_embeddings))\n",
    "indices = np.random.choice(len(pred_image_embeddings), sample_size, replace=False)\n",
    "\n",
    "# 准备t-SNE的输入数据\n",
    "combined_embeddings = np.vstack([\n",
    "    pred_image_embeddings[indices].numpy(),\n",
    "    true_image_embeddings[indices].numpy()\n",
    "])\n",
    "\n",
    "# 创建标签（0表示预测嵌入，1表示真实嵌入）\n",
    "labels = np.array([0] * sample_size + [1] * sample_size)\n",
    "\n",
    "# 执行t-SNE降维\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)  # 使用max_iter而不是n_iter\n",
    "embeddings_2d = tsne.fit_transform(combined_embeddings)\n",
    "\n",
    "# 分离预测嵌入和真实嵌入的t-SNE结果\n",
    "pred_2d = embeddings_2d[:sample_size]\n",
    "true_2d = embeddings_2d[sample_size:]\n",
    "\n",
    "# 绘制t-SNE散点图\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(pred_2d[:, 0], pred_2d[:, 1], alpha=0.5, label='Predicted Image Embeddings', s=10)\n",
    "plt.scatter(true_2d[:, 0], true_2d[:, 1], alpha=0.5, label='Real Image Embeddings', s=10)\n",
    "plt.title('t-SNE Visualization of Image Embeddings')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'tsne_visualization.png'))\n",
    "plt.close()\n",
    "\n",
    "# 绘制连线图，显示相同索引的预测和真实嵌入之间的距离\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(pred_2d[:, 0], pred_2d[:, 1], alpha=0.5, label='Predicted Image Embeddings', s=10)\n",
    "plt.scatter(true_2d[:, 0], true_2d[:, 1], alpha=0.5, label='Real Image Embeddings', s=10)\n",
    "\n",
    "# 随机选择一些点（避免图形过于拥挤）\n",
    "line_indices = np.random.choice(sample_size, 100, replace=False)\n",
    "for idx in line_indices:\n",
    "    plt.plot([pred_2d[idx, 0], true_2d[idx, 0]], \n",
    "             [pred_2d[idx, 1], true_2d[idx, 1]], \n",
    "             'k-', alpha=0.1)\n",
    "\n",
    "plt.title('t-SNE Visualization of Image Embeddings (with Connection Lines)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'tsne_visualization_with_lines.png'))\n",
    "plt.close()\n",
    "\n",
    "print(f\"Analysis complete! Results saved to {OUTPUT_DIR} directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-sne原本的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Text embeddings data type: torch.float16\n",
      "Image embeddings data type: torch.float16\n",
      "Text embeddings shape: torch.Size([162770, 512])\n",
      "Image embeddings shape: torch.Size([162770, 512])\n",
      "Converted text embeddings to float32\n",
      "Converted image embeddings to float32\n",
      "Computing cosine similarity between original text and image embeddings...\n",
      "Average cosine similarity: 0.2546 ± 0.0209\n",
      "Performing t-SNE dimensionality reduction...\n",
      "Average Euclidean distance: 11.8694 ± 0.6074\n",
      "Analysis complete! Results saved to ./original_embeddings_comparison directory\n",
      "\n",
      "Comparing similarities:\n",
      "Original text-to-image cosine similarity: 0.2546\n",
      "Model-predicted text-to-image cosine similarity: 0.8324 (from previous analysis)\n",
      "Improvement: 0.5778 (226.95%)\n"
     ]
    }
   ],
   "source": [
    "# 导入所需库\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 定义文件路径\n",
    "TEXT_EMBEDDINGS_PATH = './clip_embeddings/text_embeddings.pt'\n",
    "IMAGE_EMBEDDINGS_PATH = './clip_embeddings/image_embeddings.pt'\n",
    "OUTPUT_DIR = './original_embeddings_comparison'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 加载嵌入\n",
    "text_embeddings = torch.load(TEXT_EMBEDDINGS_PATH)\n",
    "image_embeddings = torch.load(IMAGE_EMBEDDINGS_PATH)\n",
    "\n",
    "# 检查和转换数据类型\n",
    "print(f\"Text embeddings data type: {text_embeddings.dtype}\")\n",
    "print(f\"Image embeddings data type: {image_embeddings.dtype}\")\n",
    "print(f\"Text embeddings shape: {text_embeddings.shape}\")\n",
    "print(f\"Image embeddings shape: {image_embeddings.shape}\")\n",
    "\n",
    "# 确保两者都是 float32 类型\n",
    "if text_embeddings.dtype != torch.float32:\n",
    "    text_embeddings = text_embeddings.float()\n",
    "    print(\"Converted text embeddings to float32\")\n",
    "if image_embeddings.dtype != torch.float32:\n",
    "    image_embeddings = image_embeddings.float()\n",
    "    print(\"Converted image embeddings to float32\")\n",
    "\n",
    "# 确保嵌入向量已归一化（对于余弦相似度计算）\n",
    "text_embeddings_norm = F.normalize(text_embeddings, p=2, dim=1)\n",
    "image_embeddings_norm = F.normalize(image_embeddings, p=2, dim=1)\n",
    "\n",
    "# 计算余弦相似度\n",
    "print(\"Computing cosine similarity between original text and image embeddings...\")\n",
    "cos_sim = torch.sum(text_embeddings_norm * image_embeddings_norm, dim=1)\n",
    "mean_cos_sim = cos_sim.mean().item()\n",
    "std_cos_sim = cos_sim.std().item()\n",
    "\n",
    "print(f\"Average cosine similarity: {mean_cos_sim:.4f} ± {std_cos_sim:.4f}\")\n",
    "\n",
    "# 保存相似度分布直方图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(cos_sim.numpy(), bins=50, alpha=0.7)\n",
    "plt.axvline(mean_cos_sim, color='r', linestyle='--', label=f'Mean: {mean_cos_sim:.4f}')\n",
    "plt.title('Cosine Similarity Distribution Between Original Text and Image Embeddings')\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'original_embeddings_cosine_similarity.png'))\n",
    "plt.close()\n",
    "\n",
    "# 使用t-SNE可视化嵌入\n",
    "print(\"Performing t-SNE dimensionality reduction...\")\n",
    "\n",
    "# 为了提高效率，随机采样部分数据点（如果数据集很大）\n",
    "sample_size = min(5000, len(text_embeddings))\n",
    "indices = np.random.choice(len(text_embeddings), sample_size, replace=False)\n",
    "\n",
    "# 准备t-SNE的输入数据\n",
    "combined_embeddings = np.vstack([\n",
    "    text_embeddings[indices].numpy(),\n",
    "    image_embeddings[indices].numpy()\n",
    "])\n",
    "\n",
    "# 创建标签（0表示文本嵌入，1表示图像嵌入）\n",
    "labels = np.array([0] * sample_size + [1] * sample_size)\n",
    "\n",
    "# 执行t-SNE降维\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)\n",
    "embeddings_2d = tsne.fit_transform(combined_embeddings)\n",
    "\n",
    "# 分离文本嵌入和图像嵌入的t-SNE结果\n",
    "text_2d = embeddings_2d[:sample_size]\n",
    "image_2d = embeddings_2d[sample_size:]\n",
    "\n",
    "# 绘制t-SNE散点图\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(text_2d[:, 0], text_2d[:, 1], alpha=0.5, label='Original Text Embeddings', s=10)\n",
    "plt.scatter(image_2d[:, 0], image_2d[:, 1], alpha=0.5, label='Original Image Embeddings', s=10)\n",
    "plt.title('t-SNE Visualization of Original Text and Image Embeddings')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'original_embeddings_tsne.png'))\n",
    "plt.close()\n",
    "\n",
    "# 绘制连线图，显示相同索引的文本和图像嵌入之间的距离\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(text_2d[:, 0], text_2d[:, 1], alpha=0.5, label='Original Text Embeddings', s=10)\n",
    "plt.scatter(image_2d[:, 0], image_2d[:, 1], alpha=0.5, label='Original Image Embeddings', s=10)\n",
    "\n",
    "# 随机选择一些点（避免图形过于拥挤）\n",
    "line_indices = np.random.choice(sample_size, 100, replace=False)\n",
    "for idx in line_indices:\n",
    "    plt.plot([text_2d[idx, 0], image_2d[idx, 0]], \n",
    "             [text_2d[idx, 1], image_2d[idx, 1]], \n",
    "             'k-', alpha=0.1)\n",
    "\n",
    "plt.title('t-SNE Visualization of Original Embeddings (with Connection Lines)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'original_embeddings_tsne_with_lines.png'))\n",
    "plt.close()\n",
    "\n",
    "# 计算每对嵌入之间的欧氏距离\n",
    "euclidean_distances = torch.norm(text_embeddings - image_embeddings, dim=1)\n",
    "mean_distance = euclidean_distances.mean().item()\n",
    "std_distance = euclidean_distances.std().item()\n",
    "\n",
    "print(f\"Average Euclidean distance: {mean_distance:.4f} ± {std_distance:.4f}\")\n",
    "\n",
    "# 保存欧氏距离分布直方图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(euclidean_distances.numpy(), bins=50, alpha=0.7)\n",
    "plt.axvline(mean_distance, color='r', linestyle='--', label=f'Mean: {mean_distance:.4f}')\n",
    "plt.title('Euclidean Distance Distribution Between Original Text and Image Embeddings')\n",
    "plt.xlabel('Euclidean Distance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'original_embeddings_euclidean_distance.png'))\n",
    "plt.close()\n",
    "\n",
    "print(f\"Analysis complete! Results saved to {OUTPUT_DIR} directory\")\n",
    "\n",
    "# 为了比较，计算之前分析中预测图像嵌入与真实图像嵌入之间的相似度与原始文本-图像嵌入相似度\n",
    "print(\"\\nComparing similarities:\")\n",
    "print(f\"Original text-to-image cosine similarity: {mean_cos_sim:.4f}\")\n",
    "print(f\"Model-predicted text-to-image cosine similarity: 0.8324 (from previous analysis)\")\n",
    "improvement = 0.8324 - mean_cos_sim\n",
    "print(f\"Improvement: {improvement:.4f} ({improvement/mean_cos_sim*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA的对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading embeddings...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text embeddings data type: torch.float16\n",
      "Image embeddings data type: torch.float16\n",
      "Text embeddings shape: torch.Size([162770, 512])\n",
      "Image embeddings shape: torch.Size([162770, 512])\n",
      "Converted text embeddings to float32\n",
      "Converted image embeddings to float32\n",
      "Computing cosine similarity between original text and image embeddings...\n",
      "Average cosine similarity: 0.2546 ± 0.0209\n",
      "Preparing data for PCA visualization...\n",
      "Performing PCA dimensionality reduction for original embeddings...\n",
      "PCA explained variance ratio: [0.44088757 0.04314086]\n",
      "Loading model...\n",
      "Generating predicted image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 636/636 [00:00<00:00, 1740.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing cosine similarity between predicted and real image embeddings...\n",
      "Average cosine similarity (predicted vs real): 0.8313 ± 0.0518\n",
      "Preparing data for PCA visualization...\n",
      "Performing PCA dimensionality reduction for predicted vs real embeddings...\n",
      "PCA explained variance ratio: [0.7328643  0.05078154]\n",
      "\n",
      "Comparison of similarities:\n",
      "Original text-to-image cosine similarity: 0.2546\n",
      "Predicted-to-real image cosine similarity: 0.8313\n",
      "Improvement: 0.5768 (226.54%)\n",
      "\n",
      "Visualization complete! Results saved to ./pca_visualization_results directory\n"
     ]
    }
   ],
   "source": [
    "# 导入所需库\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional as F\n",
    "from sklearn.decomposition import PCA\n",
    "import torch.nn as nn\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 定义文件路径\n",
    "TEXT_EMBEDDINGS_PATH = './clip_embeddings/text_embeddings.pt'\n",
    "IMAGE_EMBEDDINGS_PATH = './clip_embeddings/image_embeddings.pt'\n",
    "MODEL_PATH = 'text_to_image_embedder.pth'\n",
    "OUTPUT_DIR = './pca_visualization_results'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 定义映射网络模型\n",
    "class TextToImageEmbedder(nn.Module):\n",
    "    def __init__(self, clip_dim=512, embed_dim=512):\n",
    "        super(TextToImageEmbedder, self).__init__()\n",
    "        self.mapping = nn.Sequential(\n",
    "            nn.Linear(clip_dim, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, embed_dim),\n",
    "            nn.BatchNorm1d(embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, embed_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, text_embeddings):\n",
    "        image_embeddings = self.mapping(text_embeddings)\n",
    "        return image_embeddings\n",
    "\n",
    "# 加载嵌入\n",
    "print(\"Loading embeddings...\")\n",
    "text_embeddings = torch.load(TEXT_EMBEDDINGS_PATH)\n",
    "image_embeddings = torch.load(IMAGE_EMBEDDINGS_PATH)\n",
    "\n",
    "# 检查和转换数据类型\n",
    "print(f\"Text embeddings data type: {text_embeddings.dtype}\")\n",
    "print(f\"Image embeddings data type: {image_embeddings.dtype}\")\n",
    "print(f\"Text embeddings shape: {text_embeddings.shape}\")\n",
    "print(f\"Image embeddings shape: {image_embeddings.shape}\")\n",
    "\n",
    "# 确保两者都是 float32 类型\n",
    "if text_embeddings.dtype != torch.float32:\n",
    "    text_embeddings = text_embeddings.float()\n",
    "    print(\"Converted text embeddings to float32\")\n",
    "if image_embeddings.dtype != torch.float32:\n",
    "    image_embeddings = image_embeddings.float()\n",
    "    print(\"Converted image embeddings to float32\")\n",
    "\n",
    "# ------------ 第一部分：比较原始文本和图像嵌入 ------------\n",
    "\n",
    "# 确保嵌入向量已归一化（对于余弦相似度计算）\n",
    "text_embeddings_norm = F.normalize(text_embeddings, p=2, dim=1)\n",
    "image_embeddings_norm = F.normalize(image_embeddings, p=2, dim=1)\n",
    "\n",
    "# 计算余弦相似度\n",
    "print(\"Computing cosine similarity between original text and image embeddings...\")\n",
    "cos_sim = torch.sum(text_embeddings_norm * image_embeddings_norm, dim=1)\n",
    "mean_cos_sim = cos_sim.mean().item()\n",
    "std_cos_sim = cos_sim.std().item()\n",
    "\n",
    "print(f\"Average cosine similarity: {mean_cos_sim:.4f} ± {std_cos_sim:.4f}\")\n",
    "\n",
    "# 为了提高效率，随机采样部分数据点\n",
    "sample_size = min(5000, len(text_embeddings))\n",
    "indices = np.random.choice(len(text_embeddings), sample_size, replace=False)\n",
    "\n",
    "# 准备PCA的输入数据\n",
    "print(\"Preparing data for PCA visualization...\")\n",
    "original_combined_embeddings = np.vstack([\n",
    "    text_embeddings[indices].numpy(),\n",
    "    image_embeddings[indices].numpy()\n",
    "])\n",
    "\n",
    "# 执行PCA降维\n",
    "print(\"Performing PCA dimensionality reduction for original embeddings...\")\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "original_embeddings_2d = pca.fit_transform(original_combined_embeddings)\n",
    "print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "\n",
    "# 分离文本嵌入和图像嵌入的PCA结果\n",
    "text_2d = original_embeddings_2d[:sample_size]\n",
    "image_2d = original_embeddings_2d[sample_size:]\n",
    "\n",
    "# 绘制PCA散点图\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(text_2d[:, 0], text_2d[:, 1], alpha=0.5, label='Original Text Embeddings', s=10)\n",
    "plt.scatter(image_2d[:, 0], image_2d[:, 1], alpha=0.5, label='Original Image Embeddings', s=10)\n",
    "plt.title('PCA Visualization of Original Text and Image Embeddings')\n",
    "plt.xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]:.2%})')\n",
    "plt.ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]:.2%})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'original_embeddings_pca.png'))\n",
    "plt.close()\n",
    "\n",
    "# 绘制连线图，显示相同索引的文本和图像嵌入之间的距离\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(text_2d[:, 0], text_2d[:, 1], alpha=0.5, label='Original Text Embeddings', s=10)\n",
    "plt.scatter(image_2d[:, 0], image_2d[:, 1], alpha=0.5, label='Original Image Embeddings', s=10)\n",
    "\n",
    "# 随机选择一些点（避免图形过于拥挤）\n",
    "line_indices = np.random.choice(sample_size, 100, replace=False)\n",
    "for idx in line_indices:\n",
    "    plt.plot([text_2d[idx, 0], image_2d[idx, 0]], \n",
    "             [text_2d[idx, 1], image_2d[idx, 1]], \n",
    "             'k-', alpha=0.1)\n",
    "\n",
    "plt.title('PCA Visualization of Original Embeddings (with Connection Lines)')\n",
    "plt.xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]:.2%})')\n",
    "plt.ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]:.2%})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'original_embeddings_pca_with_lines.png'))\n",
    "plt.close()\n",
    "\n",
    "# ------------ 第二部分：使用映射网络生成预测图像嵌入并比较 ------------\n",
    "\n",
    "# 加载模型\n",
    "print(\"Loading model...\")\n",
    "embedder = TextToImageEmbedder().to(device)\n",
    "embedder.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "embedder.eval()\n",
    "\n",
    "# 使用训练好的映射网络生成图像嵌入\n",
    "print(\"Generating predicted image embeddings...\")\n",
    "with torch.no_grad():\n",
    "    # 分批处理以减少内存使用\n",
    "    batch_size = 256\n",
    "    pred_image_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(text_embeddings), batch_size)):\n",
    "        batch = text_embeddings[i:i+batch_size].to(device)\n",
    "        preds = embedder(batch).cpu()\n",
    "        pred_image_embeddings.append(preds)\n",
    "    \n",
    "    pred_image_embeddings = torch.cat(pred_image_embeddings, dim=0)\n",
    "\n",
    "# 确保嵌入向量已归一化\n",
    "pred_image_embeddings_norm = F.normalize(pred_image_embeddings, p=2, dim=1)\n",
    "\n",
    "# 计算预测图像嵌入与真实图像嵌入的余弦相似度\n",
    "print(\"Computing cosine similarity between predicted and real image embeddings...\")\n",
    "pred_cos_sim = torch.sum(pred_image_embeddings_norm[indices] * image_embeddings_norm[indices], dim=1)\n",
    "pred_mean_cos_sim = pred_cos_sim.mean().item()\n",
    "pred_std_cos_sim = pred_cos_sim.std().item()\n",
    "\n",
    "print(f\"Average cosine similarity (predicted vs real): {pred_mean_cos_sim:.4f} ± {pred_std_cos_sim:.4f}\")\n",
    "\n",
    "# 准备PCA的输入数据\n",
    "print(\"Preparing data for PCA visualization...\")\n",
    "predicted_combined_embeddings = np.vstack([\n",
    "    pred_image_embeddings[indices].numpy(),\n",
    "    image_embeddings[indices].numpy()\n",
    "])\n",
    "\n",
    "# 执行PCA降维\n",
    "print(\"Performing PCA dimensionality reduction for predicted vs real embeddings...\")\n",
    "pca_pred = PCA(n_components=2, random_state=42)\n",
    "predicted_embeddings_2d = pca_pred.fit_transform(predicted_combined_embeddings)\n",
    "print(f\"PCA explained variance ratio: {pca_pred.explained_variance_ratio_}\")\n",
    "\n",
    "# 分离预测图像嵌入和真实图像嵌入的PCA结果\n",
    "pred_2d = predicted_embeddings_2d[:sample_size]\n",
    "true_2d = predicted_embeddings_2d[sample_size:]\n",
    "\n",
    "# 绘制PCA散点图\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(pred_2d[:, 0], pred_2d[:, 1], alpha=0.5, label='Predicted Image Embeddings', s=10)\n",
    "plt.scatter(true_2d[:, 0], true_2d[:, 1], alpha=0.5, label='Real Image Embeddings', s=10)\n",
    "plt.title('PCA Visualization of Predicted vs Real Image Embeddings')\n",
    "plt.xlabel(f'Principal Component 1 ({pca_pred.explained_variance_ratio_[0]:.2%})')\n",
    "plt.ylabel(f'Principal Component 2 ({pca_pred.explained_variance_ratio_[1]:.2%})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'predicted_vs_real_pca.png'))\n",
    "plt.close()\n",
    "\n",
    "# 绘制连线图\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(pred_2d[:, 0], pred_2d[:, 1], alpha=0.5, label='Predicted Image Embeddings', s=10)\n",
    "plt.scatter(true_2d[:, 0], true_2d[:, 1], alpha=0.5, label='Real Image Embeddings', s=10)\n",
    "\n",
    "# 使用相同的line_indices保持一致性\n",
    "for idx in line_indices:\n",
    "    plt.plot([pred_2d[idx, 0], true_2d[idx, 0]], \n",
    "             [pred_2d[idx, 1], true_2d[idx, 1]], \n",
    "             'k-', alpha=0.1)\n",
    "\n",
    "plt.title('PCA Visualization of Predicted vs Real Image Embeddings (with Connection Lines)')\n",
    "plt.xlabel(f'Principal Component 1 ({pca_pred.explained_variance_ratio_[0]:.2%})')\n",
    "plt.ylabel(f'Principal Component 2 ({pca_pred.explained_variance_ratio_[1]:.2%})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'predicted_vs_real_pca_with_lines.png'))\n",
    "plt.close()\n",
    "\n",
    "# 比较改进情况\n",
    "improvement = pred_mean_cos_sim - mean_cos_sim\n",
    "print(\"\\nComparison of similarities:\")\n",
    "print(f\"Original text-to-image cosine similarity: {mean_cos_sim:.4f}\")\n",
    "print(f\"Predicted-to-real image cosine similarity: {pred_mean_cos_sim:.4f}\")\n",
    "print(f\"Improvement: {improvement:.4f} ({improvement/mean_cos_sim*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nVisualization complete! Results saved to {OUTPUT_DIR} directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDS的对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading embeddings...\n",
      "Text embeddings data type: torch.float16\n",
      "Image embeddings data type: torch.float16\n",
      "Text embeddings shape: torch.Size([162770, 512])\n",
      "Image embeddings shape: torch.Size([162770, 512])\n",
      "Converted text embeddings to float32\n",
      "Converted image embeddings to float32\n",
      "Loading model...\n",
      "Generating predicted image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 636/636 [00:00<00:00, 1716.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing cosine similarities...\n",
      "Original text-to-image cosine similarity: 0.2546\n",
      "Predicted-to-real image cosine similarity: 0.8324\n",
      "Improvement: 0.5778 (226.94%)\n",
      "Creating combined visualization of all embeddings...\n",
      "Performing MDS for all three embedding types...\n",
      "MDS stress: 25435.1744\n",
      "Creating visualizations with connection lines...\n",
      "Creating distance distribution histograms...\n",
      "Visualization complete! Results saved to ./improved_mds_visualization directory\n"
     ]
    }
   ],
   "source": [
    "# 导入所需库\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional as F\n",
    "from sklearn.manifold import MDS\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 定义文件路径\n",
    "TEXT_EMBEDDINGS_PATH = './clip_embeddings/text_embeddings.pt'\n",
    "IMAGE_EMBEDDINGS_PATH = './clip_embeddings/image_embeddings.pt'\n",
    "MODEL_PATH = 'text_to_image_embedder.pth'\n",
    "OUTPUT_DIR = './improved_mds_visualization'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 定义映射网络模型\n",
    "class TextToImageEmbedder(nn.Module):\n",
    "    def __init__(self, clip_dim=512, embed_dim=512):\n",
    "        super(TextToImageEmbedder, self).__init__()\n",
    "        self.mapping = nn.Sequential(\n",
    "            nn.Linear(clip_dim, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, embed_dim),\n",
    "            nn.BatchNorm1d(embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, embed_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, text_embeddings):\n",
    "        image_embeddings = self.mapping(text_embeddings)\n",
    "        return image_embeddings\n",
    "\n",
    "# 加载嵌入\n",
    "print(\"Loading embeddings...\")\n",
    "text_embeddings = torch.load(TEXT_EMBEDDINGS_PATH)\n",
    "image_embeddings = torch.load(IMAGE_EMBEDDINGS_PATH)\n",
    "\n",
    "# 检查和转换数据类型\n",
    "print(f\"Text embeddings data type: {text_embeddings.dtype}\")\n",
    "print(f\"Image embeddings data type: {image_embeddings.dtype}\")\n",
    "print(f\"Text embeddings shape: {text_embeddings.shape}\")\n",
    "print(f\"Image embeddings shape: {image_embeddings.shape}\")\n",
    "\n",
    "# 确保两者都是 float32 类型\n",
    "if text_embeddings.dtype != torch.float32:\n",
    "    text_embeddings = text_embeddings.float()\n",
    "    print(\"Converted text embeddings to float32\")\n",
    "if image_embeddings.dtype != torch.float32:\n",
    "    image_embeddings = image_embeddings.float()\n",
    "    print(\"Converted image embeddings to float32\")\n",
    "\n",
    "# 确保嵌入向量已归一化（对于余弦相似度计算）\n",
    "text_embeddings_norm = F.normalize(text_embeddings, p=2, dim=1)\n",
    "image_embeddings_norm = F.normalize(image_embeddings, p=2, dim=1)\n",
    "\n",
    "# 加载模型并生成预测嵌入\n",
    "print(\"Loading model...\")\n",
    "embedder = TextToImageEmbedder().to(device)\n",
    "embedder.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "embedder.eval()\n",
    "\n",
    "# 使用训练好的映射网络生成图像嵌入\n",
    "print(\"Generating predicted image embeddings...\")\n",
    "with torch.no_grad():\n",
    "    # 分批处理以减少内存使用\n",
    "    batch_size = 256\n",
    "    pred_image_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(text_embeddings), batch_size)):\n",
    "        batch = text_embeddings[i:i+batch_size].to(device)\n",
    "        preds = embedder(batch).cpu()\n",
    "        pred_image_embeddings.append(preds)\n",
    "    \n",
    "    pred_image_embeddings = torch.cat(pred_image_embeddings, dim=0)\n",
    "\n",
    "# 确保预测嵌入向量已归一化\n",
    "pred_image_embeddings_norm = F.normalize(pred_image_embeddings, p=2, dim=1)\n",
    "\n",
    "# 计算余弦相似度\n",
    "print(\"Computing cosine similarities...\")\n",
    "orig_cos_sim = torch.sum(text_embeddings_norm * image_embeddings_norm, dim=1)\n",
    "pred_cos_sim = torch.sum(pred_image_embeddings_norm * image_embeddings_norm, dim=1)\n",
    "\n",
    "orig_mean_cos_sim = orig_cos_sim.mean().item()\n",
    "pred_mean_cos_sim = pred_cos_sim.mean().item()\n",
    "\n",
    "print(f\"Original text-to-image cosine similarity: {orig_mean_cos_sim:.4f}\")\n",
    "print(f\"Predicted-to-real image cosine similarity: {pred_mean_cos_sim:.4f}\")\n",
    "improvement = pred_mean_cos_sim - orig_mean_cos_sim\n",
    "print(f\"Improvement: {improvement:.4f} ({improvement/orig_mean_cos_sim*100:.2f}%)\")\n",
    "\n",
    "# 为了更好的可视化效果，增加采样点数\n",
    "sample_size = 800  # 使用800个点来平衡计算速度和视觉效果\n",
    "indices = np.random.choice(len(text_embeddings), sample_size, replace=False)\n",
    "\n",
    "# 提取采样的嵌入\n",
    "sampled_text_embeds = text_embeddings[indices]\n",
    "sampled_image_embeds = image_embeddings[indices]\n",
    "sampled_pred_embeds = pred_image_embeddings[indices]\n",
    "\n",
    "# 设置MDS参数 - 调整以更关注整体分布重叠\n",
    "# 使用较小的eps值和更多迭代次数来提高精度\n",
    "mds_params = {\n",
    "    'n_components': 2,\n",
    "    'random_state': 42,\n",
    "    'n_init': 1,\n",
    "    'max_iter': 500,  # 增加迭代次数\n",
    "    'n_jobs': 1,\n",
    "    'dissimilarity': 'precomputed',\n",
    "    'normalized_stress': 'auto',\n",
    "    'eps': 1e-6  # 更严格的收敛标准\n",
    "}\n",
    "\n",
    "# ------------ 第一部分：所有三种嵌入的综合可视化 ------------\n",
    "print(\"Creating combined visualization of all embeddings...\")\n",
    "\n",
    "# 准备包含所有三种嵌入的数据\n",
    "all_combined_embeddings = np.vstack([\n",
    "    sampled_text_embeds.numpy(),\n",
    "    sampled_image_embeds.numpy(),\n",
    "    sampled_pred_embeds.numpy()\n",
    "])\n",
    "\n",
    "# 计算余弦距离矩阵\n",
    "cosine_dist_matrix = cosine_distances(all_combined_embeddings)\n",
    "\n",
    "# 执行MDS降维\n",
    "print(\"Performing MDS for all three embedding types...\")\n",
    "mds = MDS(**mds_params)\n",
    "all_embeddings_2d = mds.fit_transform(cosine_dist_matrix)\n",
    "print(f\"MDS stress: {mds.stress_:.4f}\")\n",
    "\n",
    "# 分离三种嵌入的MDS结果\n",
    "text_2d = all_embeddings_2d[:sample_size]\n",
    "image_2d = all_embeddings_2d[sample_size:2*sample_size]\n",
    "pred_2d = all_embeddings_2d[2*sample_size:]\n",
    "\n",
    "# 绘制包含三种嵌入的散点图\n",
    "plt.figure(figsize=(14, 12))\n",
    "plt.scatter(text_2d[:, 0], text_2d[:, 1], alpha=0.6, label='Original Text Embeddings', s=15, color='blue')\n",
    "plt.scatter(image_2d[:, 0], image_2d[:, 1], alpha=0.6, label='Real Image Embeddings', s=15, color='orange')\n",
    "plt.scatter(pred_2d[:, 0], pred_2d[:, 1], alpha=0.6, label='Predicted Image Embeddings', s=15, color='green')\n",
    "\n",
    "# 添加标题和标签\n",
    "plt.title('MDS Visualization of All Three Embedding Types', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'three_embeddings_mds.png'), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# ------------ 第二部分：原始文本和图像嵌入与预测图像嵌入的距离连线 ------------\n",
    "print(\"Creating visualizations with connection lines...\")\n",
    "\n",
    "# 绘制文本到预测图像的连线 - 显示映射效果\n",
    "plt.figure(figsize=(14, 12))\n",
    "plt.scatter(text_2d[:, 0], text_2d[:, 1], alpha=0.5, label='Original Text Embeddings', s=10, color='blue')\n",
    "plt.scatter(pred_2d[:, 0], pred_2d[:, 1], alpha=0.5, label='Predicted Image Embeddings', s=10, color='green')\n",
    "\n",
    "# 选择50个点显示连线\n",
    "line_indices = np.random.choice(sample_size, 50, replace=False)\n",
    "for idx in line_indices:\n",
    "    plt.plot([text_2d[idx, 0], pred_2d[idx, 0]], \n",
    "             [text_2d[idx, 1], pred_2d[idx, 1]], \n",
    "             'k-', alpha=0.2)\n",
    "\n",
    "plt.title('Text to Predicted Image Embedding Mapping', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'text_to_predicted_connections.png'), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# 绘制预测图像到真实图像的连线 - 显示预测与真实的差距\n",
    "plt.figure(figsize=(14, 12))\n",
    "plt.scatter(pred_2d[:, 0], pred_2d[:, 1], alpha=0.5, label='Predicted Image Embeddings', s=10, color='green')\n",
    "plt.scatter(image_2d[:, 0], image_2d[:, 1], alpha=0.5, label='Real Image Embeddings', s=10, color='orange')\n",
    "\n",
    "# 使用相同的索引绘制连线\n",
    "for idx in line_indices:\n",
    "    plt.plot([pred_2d[idx, 0], image_2d[idx, 0]], \n",
    "             [pred_2d[idx, 1], image_2d[idx, 1]], \n",
    "             'k-', alpha=0.2)\n",
    "\n",
    "plt.title('Predicted vs Real Image Embeddings', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'predicted_to_real_connections.png'), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# ------------ 第三部分：距离分布直方图 ------------\n",
    "print(\"Creating distance distribution histograms...\")\n",
    "\n",
    "# 计算欧氏距离\n",
    "text_to_image_dist = np.sqrt(np.sum((sampled_text_embeds.numpy() - sampled_image_embeds.numpy())**2, axis=1))\n",
    "pred_to_image_dist = np.sqrt(np.sum((sampled_pred_embeds.numpy() - sampled_image_embeds.numpy())**2, axis=1))\n",
    "\n",
    "# 绘制欧氏距离分布直方图\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.hist(text_to_image_dist, bins=50, alpha=0.5, label='Text to Real Image', color='blue')\n",
    "plt.hist(pred_to_image_dist, bins=50, alpha=0.5, label='Predicted to Real Image', color='green')\n",
    "plt.axvline(np.mean(text_to_image_dist), color='blue', linestyle='--', \n",
    "           label=f'Mean Text-Image: {np.mean(text_to_image_dist):.2f}')\n",
    "plt.axvline(np.mean(pred_to_image_dist), color='green', linestyle='--', \n",
    "           label=f'Mean Pred-Image: {np.mean(pred_to_image_dist):.2f}')\n",
    "plt.title('Euclidean Distance Distribution', fontsize=16)\n",
    "plt.xlabel('Distance', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'distance_distribution.png'), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"Visualization complete! Results saved to {OUTPUT_DIR} directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 映射结果的计算 我们告一段落，主要用MDS即可，然后辅助性的图可以用t-sne图，还有余弦相似度的图，还有欧几里得距离的图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用以上消息，我们接着干干嘛呢？ 接着我们就需要训练我们的模型本身内容 训练模型本身\n",
    "\n",
    "其中代码中明确注释标记了：\n",
    "\n",
    "0: train (训练集)\n",
    "\n",
    "1: val (验证集)\n",
    "\n",
    "2: test (测试集)\n",
    "\n",
    "文件对应关系\n",
    "\n",
    "因此，生成的文件对应关系如下：\n",
    "\n",
    "训练用的嵌入文件:\n",
    "\n",
    "文本嵌入: text_embeddings_partition_0.pt\n",
    "\n",
    "图像嵌入: image_embeddings_partition_0.pt\n",
    "\n",
    "验证用的嵌入文件:\n",
    "\n",
    "文本嵌入: text_embeddings_partition_1.pt\n",
    "\n",
    "图像嵌入: image_embeddings_partition_1.pt\n",
    "\n",
    "测试用的嵌入文件:\n",
    "\n",
    "文本嵌入: text_embeddings_partition_2.pt\n",
    "\n",
    "图像嵌入: image_embeddings_partition_2.pt\n",
    "\n",
    "\n",
    "所以我们本就应该用生成的图像嵌入和原本的图像作为训练的内容进行训练，也只能这样\n",
    "\n",
    "训练的代码取自realtest3_clipcvae.ipynb的倒数第二个，最后一个是在这个基础上改的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 我们在CVAE训练的基础上 ， 用 Renyi divergence 重新训练我们的模型 训练成功了 就在Renyi_test2.ipynb中 Alpha=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n",
      "CLIP模型已加载。\n",
      "加载测试数据集...\n",
      "Total samples before filtering: 202599\n",
      "Total samples after filtering partition 2: 19962\n",
      "加载KL模型...\n",
      "加载Rényi模型...\n",
      "从测试集采样1000张图像进行评估...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成评估图像:   5%|▍         | 15/312 [00:01<00:21, 14.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "采样完成，每个类别有1000张图像\n",
      "计算FID...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 28.71it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 29.39it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 29.35it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 29.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计算Inception Score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pytorc_test1/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/pytorc_test1/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计算LPIPS...\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pytorc_test1/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /root/miniconda3/envs/pytorc_test1/lib/python3.12/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /root/miniconda3/envs/pytorc_test1/lib/python3.12/site-packages/lpips/weights/v0.1/alex.pth\n",
      "\n",
      "=========== 评估结果 ===========\n",
      "CVAE-KL FID: 62.7659\n",
      "CVAE-Rényi FID: 328.9333 (改进: -424.06%)\n",
      "CVAE-KL IS: 2.2409 ± 0.1771\n",
      "CVAE-Rényi IS: 1.6552 ± 0.0376 (改进: -26.14%)\n",
      "CVAE-KL LPIPS: 0.1379\n",
      "CVAE-Rényi LPIPS: 0.3872 (改进: 180.87%)\n",
      "\n",
      "Latex表格数据:\n",
      "\\begin{tabular}{lllll}\n",
      "\\toprule\n",
      "Model & FID & IS & LPIPS & p-value \\\\\n",
      "\\midrule\n",
      "CVAE-KL & 62.7659 & 2.2409 & 0.1379 & N/A \\\\\n",
      "CVAE-Rényi (α=0.9) & 328.9333 & 1.6552 & 0.3872 & N/A \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "结果已保存到 divergence_comparison_results.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import lpips\n",
    "from scipy import stats\n",
    "\n",
    "# FID计算需要的库\n",
    "try:\n",
    "    from pytorch_fid import fid_score\n",
    "except ImportError:\n",
    "    print(\"pytorch_fid未安装，请使用 pip install pytorch-fid 安装\")\n",
    "\n",
    "# Inception Score计算需要的库\n",
    "try:\n",
    "    from torchvision.models import inception_v3\n",
    "except ImportError:\n",
    "    print(\"torchvision未安装或版本过低，请使用 pip install -U torchvision 更新\")\n",
    "\n",
    "# ------------------------ #\n",
    "# 1. 设备设置\n",
    "# ------------------------ #\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# ------------------------ #\n",
    "# 2. 加载CLIP模型\n",
    "# ------------------------ #\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "clip_model.eval()\n",
    "print(\"CLIP模型已加载。\")\n",
    "\n",
    "# ------------------------ #\n",
    "# 3. 定义CelebA数据集类（用于测试集）\n",
    "# ------------------------ #\n",
    "class CelebADataset(Dataset):\n",
    "    def __init__(self, img_dir, attr_path, bbox_path, partition_path, \n",
    "                 image_embeddings_path, transform=None, partition=2):  # 默认加载测试集\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # 读取属性文件\n",
    "        attr_df = pd.read_csv(attr_path, sep=',', header=0)\n",
    "        \n",
    "        # 读取分区文件\n",
    "        partition_df = pd.read_csv(partition_path, sep=',', header=0, names=['image_id', 'partition'])\n",
    "        \n",
    "        # 读取边界框文件\n",
    "        bbox_df = pd.read_csv(bbox_path, sep=',', header=0, names=['image_id', 'x_1', 'y_1', 'width', 'height'])\n",
    "        \n",
    "        # 合并数据\n",
    "        attr_df = attr_df.merge(partition_df, on='image_id')\n",
    "        attr_df = attr_df.merge(bbox_df, on='image_id')\n",
    "        \n",
    "        # 筛选分区\n",
    "        print(f\"Total samples before filtering: {len(attr_df)}\")\n",
    "        self.attr_df = attr_df[attr_df['partition'] == partition].reset_index(drop=True)\n",
    "        print(f\"Total samples after filtering partition {partition}: {len(self.attr_df)}\")\n",
    "\n",
    "        # 获取属性名称\n",
    "        self.attr_names = [col for col in attr_df.columns if col not in ['image_id', 'partition', 'x_1', 'y_1', 'width', 'height']]\n",
    "        \n",
    "        # 加载预计算的图像嵌入\n",
    "        self.image_embeddings = torch.load(image_embeddings_path)\n",
    "        if len(self.image_embeddings) != len(self.attr_df):\n",
    "            raise ValueError(\"图像嵌入的数量与数据集中的图像数量不一致。\")\n",
    "        self.image_embeddings = self.image_embeddings.float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.attr_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 获取图像路径\n",
    "        img_name = self.attr_df.iloc[idx, self.attr_df.columns.get_loc('image_id')]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # 获取属性标签 \n",
    "        attrs = self.attr_df.iloc[idx][self.attr_names].values\n",
    "        attrs = (attrs + 1) // 2  # 将-1转为0，1保持1\n",
    "        attrs = attrs.astype(np.float32)\n",
    "\n",
    "        # 获取对应的图像嵌入\n",
    "        clip_embedding = self.image_embeddings[idx]\n",
    "\n",
    "        return image, attrs, clip_embedding, img_name\n",
    "\n",
    "# ------------------------ #\n",
    "# 4. 定义CVAE模型（KL和Rényi）\n",
    "# ------------------------ #\n",
    "class ClipCVAE(nn.Module):\n",
    "    def __init__(self, img_channels=3, img_size=64, latent_dim=128, \n",
    "                 cond_dim=40, clip_dim=512):\n",
    "        super(ClipCVAE, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.cond_dim = cond_dim\n",
    "        self.clip_dim = clip_dim\n",
    "\n",
    "        # 编码器部分\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(img_channels + cond_dim + clip_dim, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(512*4*4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(512*4*4, latent_dim)\n",
    "\n",
    "        # 解码器部分\n",
    "        self.decoder_input = nn.Linear(latent_dim + cond_dim + clip_dim, 512*4*4)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, img_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def encode(self, x, c, clip_embedding):\n",
    "        c = c.view(c.size(0), self.cond_dim, 1, 1).repeat(1, 1, self.img_size, self.img_size)\n",
    "        clip_embedding = clip_embedding.view(clip_embedding.size(0), self.clip_dim, 1, 1).repeat(1, 1, self.img_size, self.img_size)\n",
    "        x = torch.cat([x, c, clip_embedding], dim=1)\n",
    "        x = self.encoder(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, c, clip_embedding):\n",
    "        z = torch.cat([z, c, clip_embedding], dim=1)\n",
    "        x = self.decoder_input(z)\n",
    "        x = x.view(-1, 512, 4, 4)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, c, clip_embedding):\n",
    "        mu, logvar = self.encode(x, c, clip_embedding)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decode(z, c, clip_embedding)\n",
    "        return recon_x, mu, logvar\n",
    "\n",
    "# 对于Rényi模型，使用同样的架构\n",
    "ClipCVAE_Renyi = ClipCVAE\n",
    "\n",
    "# ------------------------ #\n",
    "# 5. 评估指标函数\n",
    "# ------------------------ #\n",
    "\n",
    "# 5.1 FID评估函数\n",
    "def calculate_fid(real_images, generated_images):\n",
    "    \"\"\"\n",
    "    计算FID分数\n",
    "    :param real_images: 真实图像张量\n",
    "    :param generated_images: 生成图像张量\n",
    "    :return: FID分数\n",
    "    \"\"\"\n",
    "    # 创建临时目录\n",
    "    os.makedirs(\"temp_real\", exist_ok=True)\n",
    "    os.makedirs(\"temp_gen\", exist_ok=True)\n",
    "    \n",
    "    # 保存真实图像\n",
    "    for i, img in enumerate(real_images):\n",
    "        img = (img.cpu().numpy().transpose(1, 2, 0) * 0.5 + 0.5) * 255\n",
    "        img = img.astype(np.uint8)\n",
    "        Image.fromarray(img).save(f\"temp_real/img_{i}.png\")\n",
    "    \n",
    "    # 保存生成图像\n",
    "    for i, img in enumerate(generated_images):\n",
    "        img = (img.cpu().numpy().transpose(1, 2, 0) * 0.5 + 0.5) * 255\n",
    "        img = img.astype(np.uint8)\n",
    "        Image.fromarray(img).save(f\"temp_gen/img_{i}.png\")\n",
    "    \n",
    "    # 计算FID\n",
    "    fid_value = fid_score.calculate_fid_given_paths([\"temp_real\", \"temp_gen\"], 50, device, 2048)\n",
    "    \n",
    "    # 清理临时文件\n",
    "    import shutil\n",
    "    shutil.rmtree(\"temp_real\")\n",
    "    shutil.rmtree(\"temp_gen\")\n",
    "    \n",
    "    return fid_value\n",
    "\n",
    "# 5.2 Inception Score计算\n",
    "def calculate_inception_score(imgs, batch_size=32, splits=10):\n",
    "    \"\"\"\n",
    "    计算Inception Score\n",
    "    :param imgs: 图像张量 [N, 3, H, W], 范围[-1, 1]\n",
    "    :return: (mean, std) of IS\n",
    "    \"\"\"\n",
    "    # 加载预训练InceptionV3\n",
    "    inception_model = inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "    inception_model.eval()\n",
    "    \n",
    "    # 调整图像大小为299x299（Inception V3的标准输入大小）\n",
    "    resize = torch.nn.Upsample(size=(299, 299), mode='bilinear', align_corners=True)\n",
    "    \n",
    "    # 获取预测\n",
    "    N = len(imgs)\n",
    "    preds = []\n",
    "    \n",
    "    for i in range(0, N, batch_size):\n",
    "        batch = imgs[i:i+batch_size].to(device)\n",
    "        batch = (batch + 1) / 2  # 从[-1,1]转换到[0,1]\n",
    "        batch = resize(batch)  # 调整大小为299x299\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred = inception_model(batch)\n",
    "            pred = torch.nn.functional.softmax(pred, dim=1)\n",
    "            preds.append(pred.cpu().numpy())\n",
    "    \n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    \n",
    "    # 计算IS\n",
    "    scores = []\n",
    "    for k in range(splits):\n",
    "        part = preds[k * (N // splits): (k+1) * (N // splits), :]\n",
    "        kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0)))\n",
    "        kl = np.mean(np.sum(kl, 1))\n",
    "        scores.append(np.exp(kl))\n",
    "    \n",
    "    return np.mean(scores), np.std(scores)\n",
    "\n",
    "# 5.3 LPIPS感知相似度计算\n",
    "def calculate_lpips(real_images, generated_images):\n",
    "    \"\"\"\n",
    "    计算LPIPS感知相似度\n",
    "    :param real_images: 真实图像张量 [N, 3, H, W]\n",
    "    :param generated_images: 生成图像张量 [N, 3, H, W]\n",
    "    :return: 平均LPIPS分数\n",
    "    \"\"\"\n",
    "    loss_fn_alex = lpips.LPIPS(net='alex').to(device)\n",
    "    total_distance = 0.0\n",
    "    batch_size = 32\n",
    "    n_batches = len(real_images) // batch_size + (1 if len(real_images) % batch_size != 0 else 0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min(start_idx + batch_size, len(real_images))\n",
    "            \n",
    "            real_batch = real_images[start_idx:end_idx].to(device)\n",
    "            gen_batch = generated_images[start_idx:end_idx].to(device)\n",
    "            \n",
    "            distance = loss_fn_alex(real_batch, gen_batch)\n",
    "            total_distance += distance.sum().item()\n",
    "    \n",
    "    return total_distance / len(real_images)\n",
    "\n",
    "# 6. 执行评估的主函数\n",
    "def evaluate_models():\n",
    "    # 数据路径\n",
    "    img_dir = '/root/autodl-tmp/celeba_datasets/img_align_celeba/img_align_celeba'\n",
    "    attr_path = '/root/autodl-tmp/celeba_datasets/list_attr_celeba.txt'\n",
    "    bbox_path = '/root/autodl-tmp/celeba_datasets/list_bbox_celeba.txt'\n",
    "    partition_path = '/root/autodl-tmp/celeba_datasets/list_eval_partition.txt'\n",
    "    image_embeddings_test_path = '/root/autodl-tmp/clip_embeddings/image_embeddings_partition_2.pt'\n",
    "    \n",
    "    # 模型路径\n",
    "    kl_model_path = 'clip_cvae_celeba.pth'\n",
    "    renyi_model_path = 'model_checkpoints/cvae_renyi_alpha-2.0_best.pth'\n",
    "    \n",
    "    # 检查模型文件是否存在\n",
    "    if not os.path.exists(kl_model_path):\n",
    "        print(f\"KL模型文件不存在: {kl_model_path}\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(renyi_model_path):\n",
    "        print(f\"Rényi模型文件不存在: {renyi_model_path}\")\n",
    "        return\n",
    "    \n",
    "    # 数据预处理\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    # 加载测试数据集\n",
    "    print(\"加载测试数据集...\")\n",
    "    test_dataset = CelebADataset(img_dir, attr_path, bbox_path, partition_path, \n",
    "                                image_embeddings_test_path, transform=transform, partition=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # 模型参数\n",
    "    latent_dim = 128\n",
    "    cond_dim = 40\n",
    "    clip_dim = 512\n",
    "    \n",
    "    # 加载KL模型\n",
    "    print(\"加载KL模型...\")\n",
    "    kl_model = ClipCVAE(img_channels=3, img_size=64, latent_dim=latent_dim, \n",
    "                         cond_dim=cond_dim, clip_dim=clip_dim).to(device)\n",
    "    kl_model.load_state_dict(torch.load(kl_model_path))\n",
    "    kl_model.eval()\n",
    "    \n",
    "    # 加载Rényi模型\n",
    "    print(\"加载Rényi模型...\")\n",
    "    renyi_model = ClipCVAE_Renyi(img_channels=3, img_size=64, latent_dim=latent_dim, \n",
    "                                 cond_dim=cond_dim, clip_dim=clip_dim).to(device)\n",
    "    try:\n",
    "        # 尝试直接加载状态字典\n",
    "        renyi_model.load_state_dict(torch.load(renyi_model_path))\n",
    "    except:\n",
    "        # 如果失败，尝试从checkpoint中加载\n",
    "        checkpoint = torch.load(renyi_model_path)\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            renyi_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            print(\"无法加载Rényi模型，检查文件格式\")\n",
    "            return\n",
    "    renyi_model.eval()\n",
    "    \n",
    "    # 采样图像用于评估（限制样本数量以加快计算）\n",
    "    n_samples = 1000  # 使用1000张图像进行评估\n",
    "    print(f\"从测试集采样{n_samples}张图像进行评估...\")\n",
    "    \n",
    "    # 收集真实图像和两个模型生成的图像\n",
    "    real_images = []\n",
    "    kl_generated_images = []\n",
    "    renyi_generated_images = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, attrs, clip_emb, _ in tqdm(test_loader, desc=\"生成评估图像\"):\n",
    "            if len(real_images) >= n_samples:\n",
    "                break\n",
    "                \n",
    "            # 移动数据到设备\n",
    "            data = data.to(device)\n",
    "            attrs = attrs.to(device)\n",
    "            clip_emb = clip_emb.to(device)\n",
    "            \n",
    "            # 生成重构图像\n",
    "            kl_recon, _, _ = kl_model(data, attrs, clip_emb)\n",
    "            renyi_recon, _, _ = renyi_model(data, attrs, clip_emb)\n",
    "            \n",
    "            # 添加到列表\n",
    "            real_images.append(data.cpu())\n",
    "            kl_generated_images.append(kl_recon.cpu())\n",
    "            renyi_generated_images.append(renyi_recon.cpu())\n",
    "            \n",
    "            if len(real_images) * data.size(0) >= n_samples:\n",
    "                break\n",
    "    \n",
    "    # 合并batch\n",
    "    real_images = torch.cat(real_images, dim=0)[:n_samples]\n",
    "    kl_generated_images = torch.cat(kl_generated_images, dim=0)[:n_samples]\n",
    "    renyi_generated_images = torch.cat(renyi_generated_images, dim=0)[:n_samples]\n",
    "    \n",
    "    print(f\"采样完成，每个类别有{len(real_images)}张图像\")\n",
    "    \n",
    "    # 计算FID\n",
    "    print(\"计算FID...\")\n",
    "    kl_fid = calculate_fid(real_images, kl_generated_images)\n",
    "    renyi_fid = calculate_fid(real_images, renyi_generated_images)\n",
    "    \n",
    "    # 计算IS\n",
    "    print(\"计算Inception Score...\")\n",
    "    kl_is, kl_is_std = calculate_inception_score(kl_generated_images)\n",
    "    renyi_is, renyi_is_std = calculate_inception_score(renyi_generated_images)\n",
    "    \n",
    "    # 计算LPIPS\n",
    "    print(\"计算LPIPS...\")\n",
    "    kl_lpips = calculate_lpips(real_images, kl_generated_images)\n",
    "    renyi_lpips = calculate_lpips(real_images, renyi_generated_images)\n",
    "    \n",
    "    # 计算改进百分比\n",
    "    fid_improvement = ((kl_fid - renyi_fid) / kl_fid) * 100\n",
    "    is_improvement = ((renyi_is - kl_is) / kl_is) * 100\n",
    "    lpips_improvement = ((renyi_lpips - kl_lpips) / kl_lpips) * 100\n",
    "    \n",
    "    # 输出结果\n",
    "    print(\"\\n=========== 评估结果 ===========\")\n",
    "    print(f\"CVAE-KL FID: {kl_fid:.4f}\")\n",
    "    print(f\"CVAE-Rényi FID: {renyi_fid:.4f} (改进: {fid_improvement:.2f}%)\")\n",
    "    print(f\"CVAE-KL IS: {kl_is:.4f} ± {kl_is_std:.4f}\")\n",
    "    print(f\"CVAE-Rényi IS: {renyi_is:.4f} ± {renyi_is_std:.4f} (改进: {is_improvement:.2f}%)\")\n",
    "    print(f\"CVAE-KL LPIPS: {kl_lpips:.4f}\")\n",
    "    print(f\"CVAE-Rényi LPIPS: {renyi_lpips:.4f} (改进: {lpips_improvement:.2f}%)\")\n",
    "    \n",
    "    # 创建表格数据\n",
    "    table_data = {\n",
    "        'Model': ['CVAE-KL', 'CVAE-Rényi (α=0.9)'],\n",
    "        'FID': [f\"{kl_fid:.4f}\", f\"{renyi_fid:.4f}\"],\n",
    "        'IS': [f\"{kl_is:.4f}\", f\"{renyi_is:.4f}\"],\n",
    "        'LPIPS': [f\"{kl_lpips:.4f}\", f\"{renyi_lpips:.4f}\"],\n",
    "        'p-value': ['N/A', 'N/A']  # 由于只有一次运行，无法计算p值\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(table_data)\n",
    "    print(\"\\nLatex表格数据:\")\n",
    "    print(df.to_latex(index=False))\n",
    "    \n",
    "    # 保存结果\n",
    "    df.to_csv(\"divergence_comparison_results.csv\", index=False)\n",
    "    print(\"结果已保存到 divergence_comparison_results.csv\")\n",
    "    \n",
    "    # 返回结果\n",
    "    return {\n",
    "        'kl_fid': kl_fid,\n",
    "        'renyi_fid': renyi_fid,\n",
    "        'kl_is': kl_is,\n",
    "        'renyi_is': renyi_is,\n",
    "        'kl_lpips': kl_lpips,\n",
    "        'renyi_lpips': renyi_lpips\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 执行评估\n",
    "    results = evaluate_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上述结果表明renyi alpha=0.9 效果不如KL divergence 诶\n",
    "所以我们现在有了0.5 0.7 0.9的FID的值，去评价不同指标，这样也比较好写。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如何做插值分析？\n",
    "我们需要什么，一个训练好的模型函数，一个生成函数，pdf。训练的收敛曲线，"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorc_test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
