{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 1272/1272 [01:03<00:00, 20.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 1272/1272 [01:02<00:00, 20.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 1272/1272 [01:01<00:00, 20.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 1272/1272 [01:02<00:00, 20.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 1272/1272 [01:02<00:00, 20.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 1272/1272 [01:02<00:00, 20.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 1272/1272 [01:02<00:00, 20.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 1272/1272 [01:01<00:00, 20.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 1272/1272 [01:01<00:00, 20.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 1272/1272 [01:01<00:00, 20.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 1272/1272 [01:00<00:00, 20.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 1272/1272 [01:00<00:00, 21.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 1272/1272 [01:00<00:00, 20.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 1272/1272 [01:00<00:00, 21.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 1272/1272 [01:00<00:00, 21.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 1272/1272 [01:00<00:00, 21.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 1272/1272 [01:00<00:00, 20.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 1272/1272 [01:00<00:00, 21.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 1272/1272 [01:00<00:00, 20.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 1272/1272 [01:01<00:00, 20.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n",
      "模型已保存到 /path/to/your/new/directory/clip_cvae_celeba_renyi.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pytorc_test1/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 24341 (\\N{CJK UNIFIED IDEOGRAPH-5F15}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/root/miniconda3/envs/pytorc_test1/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 23548 (\\N{CJK UNIFIED IDEOGRAPH-5BFC}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/root/miniconda3/envs/pytorc_test1/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 30340 (\\N{CJK UNIFIED IDEOGRAPH-7684}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/root/miniconda3/envs/pytorc_test1/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 26465 (\\N{CJK UNIFIED IDEOGRAPH-6761}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/root/miniconda3/envs/pytorc_test1/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 20214 (\\N{CJK UNIFIED IDEOGRAPH-4EF6}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/root/miniconda3/envs/pytorc_test1/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 29983 (\\N{CJK UNIFIED IDEOGRAPH-751F}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/root/miniconda3/envs/pytorc_test1/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 25104 (\\N{CJK UNIFIED IDEOGRAPH-6210}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/root/miniconda3/envs/pytorc_test1/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 20154 (\\N{CJK UNIFIED IDEOGRAPH-4EBA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/root/miniconda3/envs/pytorc_test1/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 33080 (\\N{CJK UNIFIED IDEOGRAPH-8138}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/root/miniconda3/envs/pytorc_test1/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 22270 (\\N{CJK UNIFIED IDEOGRAPH-56FE}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/root/miniconda3/envs/pytorc_test1/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 20687 (\\N{CJK UNIFIED IDEOGRAPH-50CF}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import clip\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# 设备选择\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 加载CLIP模型\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# 文本提示词\n",
    "TEXT_PROMPTS = [\n",
    "    \"A portrait of a young woman\",\n",
    "    \"A realistic face with a smile\",\n",
    "    \"A person with distinct facial features\"\n",
    "]\n",
    "\n",
    "# 生成CLIP文本嵌入（提前计算以避免重复计算）\n",
    "def generate_text_embeddings(text_prompts):\n",
    "    text_tokens = clip.tokenize(text_prompts).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_embeddings = clip_model.encode_text(text_tokens)\n",
    "    return text_embeddings.cpu()  # 移动到CPU\n",
    "\n",
    "# 预先计算文本嵌入\n",
    "CLIP_TEXT_EMBEDDINGS = generate_text_embeddings(TEXT_PROMPTS)\n",
    "\n",
    "# CelebA数据集类\n",
    "class CelebADataset(Dataset):\n",
    "    def __init__(self, img_dir, attr_path, bbox_path, partition_path, \n",
    "                 transform=None, partition=0):\n",
    "        \"\"\"\n",
    "        初始化数据集\n",
    "        :param img_dir: 图像文件夹路径\n",
    "        :param attr_path: 属性文件路径\n",
    "        :param bbox_path: 边界框文件路径\n",
    "        :param partition_path: 分区文件路径\n",
    "        :param transform: 图像预处理\n",
    "        :param partition: 使用的数据分区 (0: train, 1: val, 2: test)\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # 读取属性文件\n",
    "        attr_df = pd.read_csv(attr_path, sep=',', header=0)\n",
    "        partition_df = pd.read_csv(partition_path, sep=',', header=0)\n",
    "        \n",
    "        # 合并属性文件和分区文件\n",
    "        attr_df = attr_df.merge(partition_df, on='image_id')\n",
    "        \n",
    "        # 根据指定的分区进行筛选\n",
    "        self.attr_df = attr_df[attr_df['partition'] == partition]\n",
    "        \n",
    "        # 读取边界框文件\n",
    "        bbox_df = pd.read_csv(bbox_path, sep=',', header=0)\n",
    "        \n",
    "        # 合并边界框信息\n",
    "        self.attr_df = self.attr_df.merge(bbox_df, on='image_id')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.attr_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 获取图像文件名\n",
    "        img_name = self.attr_df.iloc[idx, 0]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        \n",
    "        # 打开图像并转换为RGB\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # 获取属性标签 \n",
    "        attrs = self.attr_df.iloc[idx, 1:41].values\n",
    "        attrs = (attrs + 1) // 2  # 将-1转为0，1保持1\n",
    "        attrs = attrs.astype(np.float32)\n",
    "        \n",
    "        # 应用图像预处理\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # 随机选择一个CLIP文本嵌入\n",
    "        random_idx = random.randint(0, len(CLIP_TEXT_EMBEDDINGS) - 1)\n",
    "        clip_embedding = CLIP_TEXT_EMBEDDINGS[random_idx]\n",
    "        \n",
    "        return image, attrs, clip_embedding\n",
    "\n",
    "class ClipCVAE(nn.Module):\n",
    "    def __init__(self, img_channels=3, img_size=64, latent_dim=128, \n",
    "                 cond_dim=40, clip_dim=512):\n",
    "        super(ClipCVAE, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.cond_dim = cond_dim\n",
    "        self.clip_dim = clip_dim\n",
    "\n",
    "        # 编码器部分\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(img_channels + cond_dim + clip_dim, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(512*4*4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(512*4*4, latent_dim)\n",
    "\n",
    "        # 解码器部分\n",
    "        self.decoder_input = nn.Linear(latent_dim + cond_dim + clip_dim, 512*4*4)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, img_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def encode(self, x, c, clip_embedding):\n",
    "        # 将条件标签、图像和CLIP嵌入拼接\n",
    "        c = c.view(c.size(0), self.cond_dim, 1, 1).repeat(1, 1, self.img_size, self.img_size)\n",
    "        clip_embedding = clip_embedding.view(clip_embedding.size(0), self.clip_dim, 1, 1).repeat(1, 1, self.img_size, self.img_size)\n",
    "        x = torch.cat([x, c, clip_embedding], dim=1)\n",
    "        x = self.encoder(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, c, clip_embedding):\n",
    "        # 将潜在变量、条件标签和CLIP嵌入拼接\n",
    "        z = torch.cat([z, c, clip_embedding], dim=1)\n",
    "        x = self.decoder_input(z)\n",
    "        x = x.view(-1, 512, 4, 4)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, c, clip_embedding):\n",
    "        mu, logvar = self.encode(x, c, clip_embedding)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decode(z, c, clip_embedding)\n",
    "        return recon_x, mu, logvar\n",
    "\n",
    "# 图像预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # 修正归一化\n",
    "])\n",
    "\n",
    "# 数据集路径设置（请根据实际路径修改）\n",
    "img_dir = '/root/autodl-tmp/celeba_datasets/img_align_celeba/img_align_celeba'\n",
    "attr_path = '/root/autodl-tmp/celeba_datasets/list_attr_celeba.txt'\n",
    "bbox_path = '/root/autodl-tmp/celeba_datasets/list_bbox_celeba.txt'\n",
    "partition_path = '/root/autodl-tmp/celeba_datasets/list_eval_partition.txt'\n",
    "\n",
    "# 创建训练集和验证集\n",
    "train_dataset = CelebADataset(img_dir, attr_path, bbox_path, partition_path, \n",
    "                              transform=transform, partition=0)\n",
    "val_dataset = CelebADataset(img_dir, attr_path, bbox_path, partition_path, \n",
    "                            transform=transform, partition=1)\n",
    "\n",
    "# 数据加载器\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# 初始化模型、损失函数和优化器\n",
    "latent_dim = 128\n",
    "cond_dim = 40\n",
    "clip_dim = 512\n",
    "model = ClipCVAE(img_channels=3, img_size=64, latent_dim=latent_dim, \n",
    "                cond_dim=cond_dim, clip_dim=clip_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "# 定义Rényi散度参数\n",
    "alpha = 2  # 您可以根据需要调整α值\n",
    "# alpha > 1 的值 0.5 2 5\n",
    "\n",
    "# 定义损失函数\n",
    "def loss_function_renyi(recon_x, x, mu, logvar, alpha=alpha):\n",
    "    recon_loss = criterion(recon_x, x)\n",
    "    \n",
    "    # 计算Rényi散度\n",
    "    # Rényi散度的公式对于高斯分布 q(z|x)=N(mu, sigma^2) 和 p(z)=N(0,1) 为：\n",
    "    # D_alpha(q||p) = 1/(alpha -1) * log( (sigma_p / sigma_q)^alpha * exp(alpha/(2)*(mu^2 + sigma_q^2 -1)) )\n",
    "    # 其中 sigma_p = 1， sigma_q = exp(0.5 * logvar)\n",
    "    \n",
    "    sigma_q = torch.exp(0.5 * logvar)\n",
    "    sigma_p = torch.ones_like(sigma_q)  # p(z) = N(0, I)\n",
    "    \n",
    "    term1 = (sigma_p / sigma_q).pow(alpha)\n",
    "    term2 = torch.exp(alpha / 2 * (mu.pow(2) + sigma_q.pow(2) - 1))\n",
    "    renyi_div = (term1 * term2).log().sum()\n",
    "    renyi_div = renyi_div / (alpha - 1)\n",
    "    \n",
    "    return recon_loss + renyi_div\n",
    "\n",
    "# 训练模型\n",
    "def train_model(model, train_loader, val_loader, optimizer, num_epochs=50):\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, attrs, clip_emb) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\")):\n",
    "            data = data.to(device, non_blocking=True)\n",
    "            attrs = attrs.to(device, non_blocking=True)\n",
    "            clip_emb = clip_emb.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = model(data, attrs, clip_emb)\n",
    "            loss = loss_function_renyi(recon_batch, data, mu, logvar, alpha=alpha)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        \n",
    "        avg_loss = train_loss / len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch}, 平均训练损失: {avg_loss:.4f}\")\n",
    "        \n",
    "        # 验证集评估\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, attrs, clip_emb in val_loader:\n",
    "                data = data.to(device, non_blocking=True)\n",
    "                attrs = attrs.to(device, non_blocking=True)\n",
    "                clip_emb = clip_emb.to(device, non_blocking=True)\n",
    "                recon_batch, mu, logvar = model(data, attrs, clip_emb)\n",
    "                loss = loss_function_renyi(recon_batch, data, mu, logvar, alpha=alpha)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        print(f\"验证集平均损失: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 图像生成函数\n",
    "def generate_images(model, attrs, text_prompts, device, num_images=16):\n",
    "    \"\"\"\n",
    "    根据条件标签和文本提示生成图像\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_images, model.latent_dim).to(device)\n",
    "        attrs = attrs[:num_images].to(device)\n",
    "        \n",
    "        # 对于每个图像随机选择一个文本嵌入\n",
    "        text_embeddings = torch.stack([\n",
    "            CLIP_TEXT_EMBEDDINGS[random.randint(0, len(CLIP_TEXT_EMBEDDINGS) - 1)] \n",
    "            for _ in range(num_images)\n",
    "        ]).to(device)\n",
    "        \n",
    "        generated = model.decode(z, attrs, text_embeddings)\n",
    "        generated = generated.cpu()\n",
    "        return generated\n",
    "\n",
    "# 训练模型（修改训练周期为20，根据需要调整）\n",
    "trained_model = train_model(model, train_loader, val_loader, optimizer, num_epochs=20)\n",
    "\n",
    "# 保存模型到新的路径（请根据需要修改路径）\n",
    "new_model_save_path = '/path/to/your/new/directory/clip_cvae_celeba_renyi.pth'\n",
    "os.makedirs(os.path.dirname(new_model_save_path), exist_ok=True)  # 确保目录存在\n",
    "torch.save(trained_model.state_dict(), new_model_save_path)\n",
    "print(f\"模型已保存到 {new_model_save_path}\")\n",
    "\n",
    "# 生成图像示例\n",
    "data_iter = iter(val_loader)\n",
    "images, attrs, _ = next(data_iter)\n",
    "sample_attrs = attrs[:16]\n",
    "\n",
    "generated_images = generate_images(trained_model, sample_attrs, TEXT_PROMPTS, device, num_images=16)\n",
    "\n",
    "# 可视化生成的图像\n",
    "def show_images(images, title=\"Generated Images\"):\n",
    "    images = images * 0.5 + 0.5  # 反归一化\n",
    "    grid = torchvision.utils.make_grid(images, nrow=4)\n",
    "    np_grid = grid.numpy()\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(np.transpose(np_grid, (1, 2, 0)))\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_images(generated_images, title=\"CLIP引导的条件生成人脸图像\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 1272/1272 [01:03<00:00, 20.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 1272/1272 [01:04<00:00, 19.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 1272/1272 [01:03<00:00, 19.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 1272/1272 [01:04<00:00, 19.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 1272/1272 [01:03<00:00, 19.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 1272/1272 [01:03<00:00, 20.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 1272/1272 [01:04<00:00, 19.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 1272/1272 [01:03<00:00, 19.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 1272/1272 [01:03<00:00, 20.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 1272/1272 [01:04<00:00, 19.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 1272/1272 [01:04<00:00, 19.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 1272/1272 [01:04<00:00, 19.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 1272/1272 [01:03<00:00, 20.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 1272/1272 [01:02<00:00, 20.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 1272/1272 [01:31<00:00, 13.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 1272/1272 [01:31<00:00, 13.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 1272/1272 [01:31<00:00, 13.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, 平均训练损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均损失: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20:  96%|█████████▋| 1226/1272 [01:19<00:02, 15.51it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 277\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generated\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m# 训练模型（修改训练周期为20，根据需要调整）\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# 保存模型到新的路径（请根据需要修改路径）\u001b[39;00m\n\u001b[1;32m    280\u001b[0m new_model_save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/path/to/your/new/directory/clip_cvae_celeba_renyi.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[5], line 232\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, optimizer, num_epochs, alpha)\u001b[0m\n\u001b[1;32m    230\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_function_renyi(recon_batch, data, mu, logvar, alpha\u001b[38;5;241m=\u001b[39malpha)\n\u001b[1;32m    231\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 232\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    235\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import clip\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# 设备选择\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 加载CLIP模型\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# 文本提示词\n",
    "TEXT_PROMPTS = [\n",
    "    \"A portrait of a young woman\",\n",
    "    \"A realistic face with a smile\",\n",
    "    \"A person with distinct facial features\"\n",
    "]\n",
    "\n",
    "# 生成CLIP文本嵌入（提前计算以避免重复计算）\n",
    "def generate_text_embeddings(text_prompts):\n",
    "    text_tokens = clip.tokenize(text_prompts).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_embeddings = clip_model.encode_text(text_tokens)\n",
    "    return text_embeddings.cpu()  # 移动到CPU\n",
    "\n",
    "# 预先计算文本嵌入\n",
    "CLIP_TEXT_EMBEDDINGS = generate_text_embeddings(TEXT_PROMPTS)\n",
    "\n",
    "# CelebA数据集类\n",
    "class CelebADataset(Dataset):\n",
    "    def __init__(self, img_dir, attr_path, bbox_path, partition_path, \n",
    "                 transform=None, partition=0):\n",
    "        \"\"\"\n",
    "        初始化数据集\n",
    "        :param img_dir: 图像文件夹路径\n",
    "        :param attr_path: 属性文件路径\n",
    "        :param bbox_path: 边界框文件路径\n",
    "        :param partition_path: 分区文件路径\n",
    "        :param transform: 图像预处理\n",
    "        :param partition: 使用的数据分区 (0: train, 1: val, 2: test)\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # 读取属性文件\n",
    "        attr_df = pd.read_csv(attr_path, sep=',', header=0)\n",
    "        partition_df = pd.read_csv(partition_path, sep=',', header=0)\n",
    "        \n",
    "        # 合并属性文件和分区文件\n",
    "        attr_df = attr_df.merge(partition_df, on='image_id')\n",
    "        \n",
    "        # 根据指定的分区进行筛选\n",
    "        self.attr_df = attr_df[attr_df['partition'] == partition]\n",
    "        \n",
    "        # 读取边界框文件\n",
    "        bbox_df = pd.read_csv(bbox_path, sep=',', header=0)\n",
    "        \n",
    "        # 合并边界框信息\n",
    "        self.attr_df = self.attr_df.merge(bbox_df, on='image_id')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.attr_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 获取图像文件名\n",
    "        img_name = self.attr_df.iloc[idx, 0]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        \n",
    "        # 打开图像并转换为RGB\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # 获取属性标签 \n",
    "        attrs = self.attr_df.iloc[idx, 1:41].values\n",
    "        attrs = (attrs + 1) // 2  # 将-1转为0，1保持1\n",
    "        attrs = attrs.astype(np.float32)\n",
    "        \n",
    "        # 应用图像预处理\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # 随机选择一个CLIP文本嵌入\n",
    "        random_idx = random.randint(0, len(CLIP_TEXT_EMBEDDINGS) - 1)\n",
    "        clip_embedding = CLIP_TEXT_EMBEDDINGS[random_idx]\n",
    "        \n",
    "        return image, attrs, clip_embedding\n",
    "\n",
    "class ClipCVAE(nn.Module):\n",
    "    def __init__(self, img_channels=3, img_size=64, latent_dim=128, \n",
    "                 cond_dim=40, clip_dim=512):\n",
    "        super(ClipCVAE, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.cond_dim = cond_dim\n",
    "        self.clip_dim = clip_dim\n",
    "\n",
    "        # 编码器部分\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(img_channels + cond_dim + clip_dim, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(512*4*4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(512*4*4, latent_dim)\n",
    "\n",
    "        # 解码器部分\n",
    "        self.decoder_input = nn.Linear(latent_dim + cond_dim + clip_dim, 512*4*4)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, img_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def encode(self, x, c, clip_embedding):\n",
    "        # 将条件标签、图像和CLIP嵌入拼接\n",
    "        c = c.view(c.size(0), self.cond_dim, 1, 1).repeat(1, 1, self.img_size, self.img_size)\n",
    "        clip_embedding = clip_embedding.view(clip_embedding.size(0), self.clip_dim, 1, 1).repeat(1, 1, self.img_size, self.img_size)\n",
    "        x = torch.cat([x, c, clip_embedding], dim=1)\n",
    "        x = self.encoder(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, c, clip_embedding):\n",
    "        # 将潜在变量、条件标签和CLIP嵌入拼接\n",
    "        z = torch.cat([z, c, clip_embedding], dim=1)\n",
    "        x = self.decoder_input(z)\n",
    "        x = x.view(-1, 512, 4, 4)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, c, clip_embedding):\n",
    "        mu, logvar = self.encode(x, c, clip_embedding)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decode(z, c, clip_embedding)\n",
    "        return recon_x, mu, logvar\n",
    "\n",
    "# 图像预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # 修正归一化\n",
    "])\n",
    "\n",
    "# 数据集路径设置（请根据实际路径修改）\n",
    "img_dir = '/root/autodl-tmp/celeba_datasets/img_align_celeba/img_align_celeba'\n",
    "attr_path = '/root/autodl-tmp/celeba_datasets/list_attr_celeba.txt'\n",
    "bbox_path = '/root/autodl-tmp/celeba_datasets/list_bbox_celeba.txt'\n",
    "partition_path = '/root/autodl-tmp/celeba_datasets/list_eval_partition.txt'\n",
    "\n",
    "# 创建训练集和验证集\n",
    "train_dataset = CelebADataset(img_dir, attr_path, bbox_path, partition_path, \n",
    "                              transform=transform, partition=0)\n",
    "val_dataset = CelebADataset(img_dir, attr_path, bbox_path, partition_path, \n",
    "                            transform=transform, partition=1)\n",
    "\n",
    "# 数据加载器\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# 初始化模型、损失函数和优化器\n",
    "latent_dim = 128\n",
    "cond_dim = 40\n",
    "clip_dim = 512\n",
    "model = ClipCVAE(img_channels=3, img_size=64, latent_dim=latent_dim, \n",
    "                cond_dim=cond_dim, clip_dim=clip_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "# 定义Rényi散度参数\n",
    "alpha = 0.5  # 选择一个合适的α值，例如0.7\n",
    "\n",
    "# 定义损失函数\n",
    "def loss_function_renyi(recon_x, x, mu, logvar, alpha=alpha):\n",
    "    recon_loss = criterion(recon_x, x)\n",
    "    \n",
    "    # 计算σ_q^2\n",
    "    sigma_q_sq = torch.exp(logvar)\n",
    "    \n",
    "    # 计算 Rényi 散度\n",
    "    renyi_div = (-torch.sum(torch.log(sigma_q_sq)) + alpha * torch.sum(mu ** 2) + torch.sum(sigma_q_sq) - mu.size(1)) / (2 * (alpha - 1))\n",
    "    \n",
    "    return recon_loss + renyi_div\n",
    "\n",
    "# 训练模型\n",
    "def train_model(model, train_loader, val_loader, optimizer, num_epochs=50, alpha=0.7):\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, attrs, clip_emb) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\")):\n",
    "            data = data.to(device, non_blocking=True)\n",
    "            attrs = attrs.to(device, non_blocking=True)\n",
    "            clip_emb = clip_emb.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = model(data, attrs, clip_emb)\n",
    "            loss = loss_function_renyi(recon_batch, data, mu, logvar, alpha=alpha)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        \n",
    "        avg_loss = train_loss / len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch}, 平均训练损失: {avg_loss:.4f}\")\n",
    "        \n",
    "        # 验证集评估\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, attrs, clip_emb in val_loader:\n",
    "                data = data.to(device, non_blocking=True)\n",
    "                attrs = attrs.to(device, non_blocking=True)\n",
    "                clip_emb = clip_emb.to(device, non_blocking=True)\n",
    "                recon_batch, mu, logvar = model(data, attrs, clip_emb)\n",
    "                loss = loss_function_renyi(recon_batch, data, mu, logvar, alpha=alpha)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        print(f\"验证集平均损失: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 图像生成函数\n",
    "def generate_images(model, attrs, text_prompts, device, num_images=16):\n",
    "    \"\"\"\n",
    "    根据条件标签和文本提示生成图像\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_images, model.latent_dim).to(device)\n",
    "        attrs = attrs[:num_images].to(device)\n",
    "        \n",
    "        # 对于每个图像随机选择一个文本嵌入\n",
    "        text_embeddings = torch.stack([\n",
    "            CLIP_TEXT_EMBEDDINGS[random.randint(0, len(CLIP_TEXT_EMBEDDINGS) - 1)] \n",
    "            for _ in range(num_images)\n",
    "        ]).to(device)\n",
    "        \n",
    "        generated = model.decode(z, attrs, text_embeddings)\n",
    "        generated = generated.cpu()\n",
    "        return generated\n",
    "\n",
    "# 训练模型（修改训练周期为20，根据需要调整）\n",
    "trained_model = train_model(model, train_loader, val_loader, optimizer, num_epochs=20, alpha=0.7)\n",
    "\n",
    "# 保存模型到新的路径（请根据需要修改路径）\n",
    "new_model_save_path = '/path/to/your/new/directory/clip_cvae_celeba_renyi.pth'\n",
    "os.makedirs(os.path.dirname(new_model_save_path), exist_ok=True)  # 确保目录存在\n",
    "torch.save(trained_model.state_dict(), new_model_save_path)\n",
    "print(f\"模型已保存到 {new_model_save_path}\")\n",
    "\n",
    "# 生成图像示例\n",
    "data_iter = iter(val_loader)\n",
    "images, attrs, _ = next(data_iter)\n",
    "sample_attrs = attrs[:16]\n",
    "\n",
    "generated_images = generate_images(trained_model, sample_attrs, TEXT_PROMPTS, device, num_images=16)\n",
    "\n",
    "# 可视化生成的图像\n",
    "def show_images(images, title=\"Generated Images\"):\n",
    "    images = images * 0.5 + 0.5  # 反归一化\n",
    "    grid = torchvision.utils.make_grid(images, nrow=4)\n",
    "    np_grid = grid.numpy()\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(np.transpose(np_grid, (1, 2, 0)))\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_images(generated_images, title=\"CLIP引导的条件生成人脸图像\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorc_test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
