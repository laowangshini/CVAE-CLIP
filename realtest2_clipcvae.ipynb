{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 环境设置与导入库\n",
    "首先，我们需要导入所有必要的库，并设置设备（CPU 或 GPU）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: 环境设置与导入库\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import clip\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# 设备选择\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 数据集类与预处理\n",
    "定义 CelebADataset 类，并设置图像预处理方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: 数据集类与预处理\n",
    "\n",
    "# CelebA数据集类\n",
    "class CelebADataset(Dataset):\n",
    "    def __init__(self, img_dir, attr_path, bbox_path, partition_path, \n",
    "                 transform=None, partition=0):\n",
    "        \"\"\"\n",
    "        初始化数据集\n",
    "        :param img_dir: 图像文件夹路径\n",
    "        :param attr_path: 属性文件路径\n",
    "        :param bbox_path: 边界框文件路径\n",
    "        :param partition_path: 分区文件路径\n",
    "        :param transform: 图像预处理\n",
    "        :param partition: 使用的数据分区 (0: train, 1: val, 2: test)\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # 读取属性文件\n",
    "        attr_df = pd.read_csv(attr_path, sep=',', header=0)\n",
    "        partition_df = pd.read_csv(partition_path, sep=',', header=0)\n",
    "        \n",
    "        # 合并属性文件和分区文件\n",
    "        attr_df = attr_df.merge(partition_df, on='image_id')\n",
    "        \n",
    "        # 根据指定的分区进行筛选\n",
    "        self.attr_df = attr_df[attr_df['partition'] == partition]\n",
    "        \n",
    "        # 读取边界框文件\n",
    "        bbox_df = pd.read_csv(bbox_path, sep=',', header=0)\n",
    "        \n",
    "        # 合并边界框信息\n",
    "        self.attr_df = self.attr_df.merge(bbox_df, on='image_id')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.attr_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 获取图像文件名\n",
    "        img_name = self.attr_df.iloc[idx, 0]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        \n",
    "        # 打开图像并转换为RGB\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # 获取属性标签 \n",
    "        attrs = self.attr_df.iloc[idx, 1:41].values\n",
    "        attrs = (attrs + 1) // 2  # 将-1转为0，1保持1\n",
    "        attrs = attrs.astype(np.float32)\n",
    "        \n",
    "        # 应用图像预处理\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # 随机选择一个CLIP文本嵌入\n",
    "        random_idx = random.randint(0, len(CLIP_TEXT_EMBEDDINGS) - 1)\n",
    "        clip_embedding = CLIP_TEXT_EMBEDDINGS[random_idx]\n",
    "        \n",
    "        return image, attrs, clip_embedding\n",
    "\n",
    "# 配置参数\n",
    "\n",
    "# 图像预处理\n",
    "IMAGE_SIZE = 224  # 从64修改为224\n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# 其他参数保持不变\n",
    "LATENT_DIM = 128\n",
    "COND_DIM = 40\n",
    "CLIP_DIM = 512\n",
    "\n",
    "# 训练参数\n",
    "LEARNING_RATE = 1e-5\n",
    "NUM_EPOCHS = 20  # 可在实验部分修改\n",
    "\n",
    "# 其他配置\n",
    "RANDOM_SEED = 42\n",
    "NUM_RUNS_FIXED_EPOCHS = 5\n",
    "EPOCH_OPTIONS = [10, 20, 30, 40, 50]\n",
    "NUM_RUNS_PER_EPOCH = 3\n",
    "\n",
    "# 设置随机种子以确保可重复性\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "\n",
    "# 图像预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # 修正归一化\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 加载CLIP模型与预计算文本嵌入\n",
    "加载 CLIP 模型并预先计算文本嵌入，以提高训练效率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: 加载CLIP模型与预计算文本嵌入\n",
    "\n",
    "# 加载CLIP模型\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# 加载另一个CLIP模型到 CPU，用于数据加载\n",
    "clip_model_cpu, preprocess_clip_cpu = clip.load(\"ViT-B/32\", device='cpu')\n",
    "\n",
    "# 文本提示词\n",
    "TEXT_PROMPTS = [\n",
    "    \"A portrait of a young woman\",\n",
    "    \"A realistic face with a smile\",\n",
    "    \"A person with distinct facial features\"\n",
    "]\n",
    "\n",
    "# 生成CLIP文本嵌入（提前计算以避免重复计算）\n",
    "def generate_text_embeddings(text_prompts):\n",
    "    text_tokens = clip.tokenize(text_prompts).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_embeddings = clip_model.encode_text(text_tokens)\n",
    "    return text_embeddings.cpu()  # 移动到CPU\n",
    "\n",
    "# 预先计算文本嵌入\n",
    "CLIP_TEXT_EMBEDDINGS = generate_text_embeddings(TEXT_PROMPTS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 数据加载器设置\n",
    "设置训练集和验证集的数据加载器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: 数据加载器设置\n",
    "\n",
    "# 数据集路径设置（请根据实际路径修改）\n",
    "img_dir = '/root/autodl-tmp/celeba_datasets/img_align_celeba/img_align_celeba'\n",
    "attr_path = '/root/autodl-tmp/celeba_datasets/list_attr_celeba.txt'\n",
    "bbox_path = '/root/autodl-tmp/celeba_datasets/list_bbox_celeba.txt'\n",
    "partition_path = '/root/autodl-tmp/celeba_datasets/list_eval_partition.txt'\n",
    "\n",
    "# 创建训练集和验证集\n",
    "train_dataset = CelebADataset(img_dir, attr_path, bbox_path, partition_path, \n",
    "                              transform=transform, partition=0)\n",
    "val_dataset = CelebADataset(img_dir, attr_path, bbox_path, partition_path, \n",
    "                            transform=transform, partition=1)\n",
    "\n",
    "# 数据加载器\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 模型定义\n",
    "定义 ClipCVAE 模型结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: 模型定义\n",
    "\n",
    "class ClipCVAE(nn.Module):\n",
    "    def __init__(self, img_channels=3, img_size=224, latent_dim=128, \n",
    "                 cond_dim=40, clip_dim=512):\n",
    "        super(ClipCVAE, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.cond_dim = cond_dim\n",
    "        self.clip_dim = clip_dim\n",
    "\n",
    "        # 对应224x224输入，4次stride=2下采样后特征图大小为14x14\n",
    "        # 编码器部分\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(img_channels + cond_dim + clip_dim, 64, kernel_size=4, stride=2, padding=1),  # 224->112\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),                                 # 112->56\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),                                # 56->28\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),                                # 28->14\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # 计算编码器输出尺寸：512 * 14 * 14 = 100352\n",
    "        enc_out_dim = 512 * 14 * 14\n",
    "        self.fc_mu = nn.Linear(enc_out_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(enc_out_dim, latent_dim)\n",
    "\n",
    "        # 解码器部分\n",
    "        # 将潜在向量映射回512*14*14的特征图\n",
    "        self.decoder_input = nn.Linear(latent_dim + cond_dim + clip_dim, 512*14*14)\n",
    "        self.decoder = nn.Sequential(\n",
    "            # 14x14 -> 28x28\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            # 28x28 -> 56x56\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            # 56x56 -> 112x112\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            # 112x112 -> 224x224\n",
    "            nn.ConvTranspose2d(64, img_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def encode(self, x, c, clip_embedding):\n",
    "        # 调整条件和clip嵌入大小\n",
    "        c = c.view(c.size(0), self.cond_dim, 1, 1).repeat(1, 1, self.img_size, self.img_size)\n",
    "        clip_embedding = clip_embedding.view(clip_embedding.size(0), self.clip_dim, 1, 1).repeat(1, 1, self.img_size, self.img_size)\n",
    "        x = torch.cat([x, c, clip_embedding], dim=1)\n",
    "        x = self.encoder(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, c, clip_embedding):\n",
    "        # 拼接潜在变量、条件和clip嵌入\n",
    "        z = torch.cat([z, c, clip_embedding], dim=1)\n",
    "        x = self.decoder_input(z)\n",
    "        # 恢复为[batch_size, 512, 14, 14]\n",
    "        x = x.view(-1, 512, 14, 14)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, c, clip_embedding):\n",
    "        mu, logvar = self.encode(x, c, clip_embedding)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decode(z, c, clip_embedding)\n",
    "        return recon_x, mu, logvar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 初始化模型、损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: 初始化模型、损失函数和优化器\n",
    "\n",
    "# 初始化模型参数\n",
    "latent_dim = 128\n",
    "cond_dim = 40\n",
    "clip_dim = 512\n",
    "model = ClipCVAE(img_channels=3, img_size=224, latent_dim=latent_dim, \n",
    "                cond_dim=cond_dim, clip_dim=clip_dim).to(device)\n",
    "\n",
    "# 优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# 损失函数\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "# 定义损失函数\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    recon_loss = criterion(recon_x, x)\n",
    "    KL = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + KL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 训练与验证函数\n",
    "定义训练和验证的函数，并记录每个epoch的损失。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: 训练与验证函数 (加入混合精度训练)\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, num_epochs=50):\n",
    "    scaler = GradScaler()  # 初始化GradScaler\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, attrs, clip_emb) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\")):\n",
    "            data = data.to(device, non_blocking=True)\n",
    "            attrs = attrs.to(device, non_blocking=True)\n",
    "            clip_emb = clip_emb.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            # 使用autocast在前向传播和损失计算中启用混合精度\n",
    "            with autocast():\n",
    "                recon_batch, mu, logvar = model(data, attrs, clip_emb)\n",
    "                loss = loss_function(recon_batch, data, mu, logvar)\n",
    "            \n",
    "            # 使用scaler进行梯度缩放并反向传播\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"Epoch {epoch}, 平均训练损失: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # 验证集评估\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            # 验证过程也可以使用autocast加速\n",
    "            for data, attrs, clip_emb in val_loader:\n",
    "                data = data.to(device, non_blocking=True)\n",
    "                attrs = attrs.to(device, non_blocking=True)\n",
    "                clip_emb = clip_emb.to(device, non_blocking=True)\n",
    "                \n",
    "                with autocast():\n",
    "                    recon_batch, mu, logvar = model(data, attrs, clip_emb)\n",
    "                    loss = loss_function(recon_batch, data, mu, logvar)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        print(f\"验证集平均损失: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    return model, train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. 图像生成与可视化函数\n",
    "定义生成图像和显示图像的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: 图像生成与可视化函数\n",
    "\n",
    "# 图像生成函数\n",
    "def generate_images(model, attrs, text_prompts, device, num_images=16):\n",
    "    \"\"\"\n",
    "    根据条件标签和文本提示生成图像\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_images, model.latent_dim).to(device)\n",
    "        attrs = attrs[:num_images].to(device)\n",
    "        \n",
    "        # 对于每个图像随机选择一个文本嵌入\n",
    "        text_embeddings = torch.stack([\n",
    "            CLIP_TEXT_EMBEDDINGS[random.randint(0, len(CLIP_TEXT_EMBEDDINGS) - 1)] \n",
    "            for _ in range(num_images)\n",
    "        ]).to(device)\n",
    "        \n",
    "        generated = model.decode(z, attrs, text_embeddings)\n",
    "        generated = generated.cpu()\n",
    "        return generated\n",
    "\n",
    "# 可视化生成的图像\n",
    "def show_images(images, title=\"Generated Images\"):\n",
    "    images = images * 0.5 + 0.5  # 反归一化\n",
    "    grid = torchvision.utils.make_grid(images, nrow=4)\n",
    "    np_grid = grid.numpy()\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(np.transpose(np_grid, (1, 2, 0)))\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. 实验1：固定轮次，多次训练并记录损失\n",
    "在固定的训练轮次下，多次训练模型，并记录每次训练的训练和验证损失，以计算平均收敛点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 9: Experiment 1 - Fixed Epochs, Multiple Trainings\n",
    "\n",
    "# import copy\n",
    "\n",
    "# # Set experiment parameters\n",
    "# fixed_num_epochs = 20  # Fixed number of training epochs\n",
    "# num_runs = 5  # Number of training runs\n",
    "\n",
    "# # Store all run losses\n",
    "# all_train_losses = []\n",
    "# all_val_losses = []\n",
    "\n",
    "# for run in range(1, num_runs + 1):\n",
    "#     print(f\"\\n=== Run {run}/{num_runs} ===\")\n",
    "    \n",
    "#     # Reinitialize the model and optimizer\n",
    "#     model_run = ClipCVAE(img_channels=3, img_size=224, latent_dim=latent_dim, \n",
    "#                         cond_dim=cond_dim, clip_dim=clip_dim).to(device)\n",
    "#     optimizer_run = optim.Adam(model_run.parameters(), lr=1e-5)\n",
    "    \n",
    "#     # Train the model\n",
    "#     trained_model, train_losses, val_losses = train_model(\n",
    "#         model_run, train_loader, val_loader, optimizer_run, num_epochs=fixed_num_epochs\n",
    "#     )\n",
    "    \n",
    "#     all_train_losses.append(train_losses)\n",
    "#     all_val_losses.append(val_losses)\n",
    "    \n",
    "#     # Free up memory\n",
    "#     del trained_model\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "# # Calculate average loss\n",
    "# avg_train_losses = np.mean(all_train_losses, axis=0)\n",
    "# avg_val_losses = np.mean(all_val_losses, axis=0)\n",
    "\n",
    "# # Plot average loss curve\n",
    "# plt.figure(figsize=(10,5))\n",
    "# plt.plot(range(1, fixed_num_epochs + 1), avg_train_losses, label='Average Training Loss')\n",
    "# plt.plot(range(1, fixed_num_epochs + 1), avg_val_losses, label='Average Validation Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title(f'Average Loss Curve ({fixed_num_epochs} Epochs) for {num_runs} Training Runs')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. 实验2：改变轮次，多次训练并寻找最佳轮次\n",
    "在多个不同的训练轮次下进行多次训练，记录每个轮次的平均损失，并分析最佳轮次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 10: 实验2 - 改变轮次，寻找最佳轮次\n",
    "\n",
    "# # 设置实验参数\n",
    "# epoch_options = [10, 20, 30, 40, 50]  # 不同的训练轮次选项\n",
    "# num_runs_per_epoch = 3  # 每个轮次进行的训练次数\n",
    "\n",
    "# # 存储不同轮次的损失\n",
    "# epoch_train_losses = {epochs: [] for epochs in epoch_options}\n",
    "# epoch_val_losses = {epochs: [] for epochs in epoch_options}\n",
    "\n",
    "# for epochs in epoch_options: \n",
    "#     print(f\"\\n=== 训练轮次: {epochs} ===\")\n",
    "#     for run in range(1, num_runs_per_epoch + 1):\n",
    "#         print(f\"--- 运行 {run}/{num_runs_per_epoch} ---\")\n",
    "        \n",
    "#         # 重新初始化模型和优化器\n",
    "#         model_run = ClipCVAE(img_channels=3, img_size=224, latent_dim=latent_dim, \n",
    "#                             cond_dim=cond_dim, clip_dim=clip_dim).to(device)\n",
    "#         optimizer_run = optim.Adam(model_run.parameters(), lr=1e-5)\n",
    "        \n",
    "#         # 训练模型\n",
    "#         trained_model, train_losses, val_losses = train_model(\n",
    "#             model_run, train_loader, val_loader, optimizer_run, num_epochs=epochs\n",
    "#         )\n",
    "        \n",
    "#         # 记录最后一个epoch的损失\n",
    "#         epoch_train_losses[epochs].append(train_losses[-1])\n",
    "#         epoch_val_losses[epochs].append(val_losses[-1])\n",
    "        \n",
    "#         # 释放显存\n",
    "#         del trained_model\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "# # 计算每个轮次的平均损失\n",
    "# avg_epoch_train_losses = {epochs: np.mean(losses) for epochs, losses in epoch_train_losses.items()}\n",
    "# avg_epoch_val_losses = {epochs: np.mean(losses) for epochs, losses in epoch_val_losses.items()}\n",
    "\n",
    "# # 绘制不同轮次的平均验证损失\n",
    "# epochs_sorted = sorted(epoch_options)\n",
    "# val_loss_means = [avg_epoch_val_losses[epochs] for epochs in epochs_sorted]\n",
    "\n",
    "# plt.figure(figsize=(10,5))\n",
    "# plt.plot(epochs_sorted, val_loss_means, marker='o', label='平均验证损失')\n",
    "# plt.xlabel('训练轮次')\n",
    "# plt.ylabel('平均验证损失')\n",
    "# plt.title('不同训练轮次下的平均验证损失')\n",
    "# plt.xticks(epochs_sorted)\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # 打印最佳轮次\n",
    "# best_epoch = epochs_sorted[np.argmin(val_loss_means)]\n",
    "# print(f\"最佳训练轮次为: {best_epoch}, 对应的平均验证损失为: {avg_epoch_val_losses[best_epoch]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. 保存模型与生成图像\n",
    "在确定最佳轮次后，可以保存最终模型并生成一些示例图像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 11: 保存模型与生成图像\n",
    "\n",
    "# # 假设最佳轮次为 `best_epoch`\n",
    "# # 这里重新训练模型一次以获得最终模型\n",
    "# best_num_epochs = 20\n",
    "# print(f\"\\n=== 使用最佳轮次 {best_num_epochs} 重新训练模型 ===\")\n",
    "\n",
    "# # 重新初始化模型和优化器\n",
    "# final_model = ClipCVAE(img_channels=3, img_size=224, latent_dim=latent_dim, \n",
    "#                       cond_dim=cond_dim, clip_dim=clip_dim).to(device)\n",
    "# final_optimizer = optim.Adam(final_model.parameters(), lr=1e-5)\n",
    "\n",
    "# # 训练模型\n",
    "# final_trained_model, final_train_losses, final_val_losses = train_model(\n",
    "#     final_model, train_loader, val_loader, final_optimizer, num_epochs=best_num_epochs\n",
    "# )\n",
    "\n",
    "# # 保存模型\n",
    "# torch.save(final_trained_model.state_dict(), f'clip_cvae_celeba_epochs_{best_num_epochs}.pth')\n",
    "# print(f\"模型已保存为: clip_cvae_celeba_epochs_{best_num_epochs}.pth\")\n",
    "\n",
    "# # 生成图像示例\n",
    "# data_iter = iter(val_loader)\n",
    "# images, attrs, _ = next(data_iter)\n",
    "# sample_attrs = attrs[:16]\n",
    "\n",
    "# generated_images = generate_images(final_trained_model, sample_attrs, TEXT_PROMPTS, device, num_images=16)\n",
    "\n",
    "# # 可视化生成的图像\n",
    "# show_images(generated_images, title=\"CLIP引导的条件生成人脸图像\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用映射网络转化image_embeddings生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4716/1951376994.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embedder.load_state_dict(torch.load('text_to_image_embedder.pth', map_location=device))\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'final_trained_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 64\u001b[0m\n\u001b[1;32m     61\u001b[0m images, attrs, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(data_iter)\n\u001b[1;32m     62\u001b[0m sample_attrs \u001b[38;5;241m=\u001b[39m attrs[:\u001b[38;5;241m16\u001b[39m]\n\u001b[0;32m---> 64\u001b[0m generated_images \u001b[38;5;241m=\u001b[39m generate_images(\u001b[43mfinal_trained_model\u001b[49m, sample_attrs, TEXT_PROMPTS, device, num_images\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# 可视化生成的图像\u001b[39;00m\n\u001b[1;32m     67\u001b[0m show_images(generated_images, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIP映射后的条件生成人脸图像\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'final_trained_model' is not defined"
     ]
    }
   ],
   "source": [
    "# # 假设 text_to_image_embedder.pth 是已训练好的映射网络权重文件\n",
    "# # 首先在和CVAE生成图像同一个脚本中加载映射网络\n",
    "\n",
    "# # 请确保与CVAE定义在同一代码块或在此处重新定义TextToImageEmbedder类\n",
    "# class TextToImageEmbedder(nn.Module):\n",
    "#     def __init__(self, clip_dim=512, embed_dim=512):\n",
    "#         super(TextToImageEmbedder, self).__init__()\n",
    "#         self.mapping = nn.Sequential(\n",
    "#             nn.Linear(clip_dim, 1024),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(1024, embed_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(embed_dim, embed_dim)\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, text_embeddings):\n",
    "#         image_embeddings = self.mapping(text_embeddings)\n",
    "#         return image_embeddings\n",
    "\n",
    "# # 加载已训练的映射模型\n",
    "# embedder = TextToImageEmbedder(clip_dim=512, embed_dim=512).to(device)\n",
    "# embedder.load_state_dict(torch.load('text_to_image_embedder.pth', map_location=device))\n",
    "# embedder.eval()\n",
    "\n",
    "# # 修改后的 generate_images 函数\n",
    "# def generate_images(model, attrs, text_prompts, device, num_images=16):\n",
    "#     \"\"\"\n",
    "#     根据条件标签和文本提示生成图像，这里将文本嵌入映射为图像嵌入后再生成。\n",
    "#     \"\"\"\n",
    "#     model.eval()\n",
    "    \n",
    "#     # 使用CLIP模型对传入的 text_prompts 进行编码（如果text_prompts本身就是已处理好的文本嵌入可跳过此步）\n",
    "#     text_tokens = clip.tokenize(text_prompts).to(device)\n",
    "#     with torch.no_grad():\n",
    "#         raw_text_embeddings = clip_model.encode_text(text_tokens)\n",
    "#     raw_text_embeddings = raw_text_embeddings.float()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         z = torch.randn(num_images, model.latent_dim).to(device)\n",
    "#         attrs = attrs[:num_images].to(device)\n",
    "        \n",
    "#         # 对每个生成图像随机选择一个文本提示对应的嵌入，然后映射为图像嵌入\n",
    "#         chosen_img_embs = []\n",
    "#         for i in range(num_images):\n",
    "#             # 随机从文本集合中选取一个文本嵌入\n",
    "#             random_index = random.randint(0, len(text_prompts) - 1)\n",
    "#             selected_text_emb = raw_text_embeddings[random_index].unsqueeze(0)  # shape: (1, 512)\n",
    "            \n",
    "#             # 通过映射网络将文本嵌入转为图像嵌入\n",
    "#             mapped_img_emb = embedder(selected_text_emb) # shape: (1, 512)\n",
    "#             chosen_img_embs.append(mapped_img_emb)\n",
    "        \n",
    "#         chosen_img_embs = torch.cat(chosen_img_embs, dim=0) # shape: (num_images, 512)\n",
    "        \n",
    "#         generated = model.decode(z, attrs, chosen_img_embs)\n",
    "#         generated = generated.cpu()\n",
    "#         return generated\n",
    "\n",
    "# # 测试新的生成过程\n",
    "# data_iter = iter(val_loader)\n",
    "# images, attrs, _ = next(data_iter)\n",
    "# sample_attrs = attrs[:16]\n",
    "\n",
    "# generated_images = generate_images(final_trained_model, sample_attrs, TEXT_PROMPTS, device, num_images=16)\n",
    "\n",
    "# # 可视化生成的图像\n",
    "# show_images(generated_images, title=\"CLIP映射后的条件生成人脸图像\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorc_test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
